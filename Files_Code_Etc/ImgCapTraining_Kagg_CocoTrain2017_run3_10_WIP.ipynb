{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "23dZtOGmFhQB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  lib  working\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RuFZIchrFhSp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco-2017-dataset\t     thesis-imgcap-run2-deterministic-info\r\n",
      "imgcap-kagg-run3-weights-in\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptions_test.pkl\t\t       img_encodings_train_97000.pkl\r\n",
      "descriptions_train_97000.pkl\t       img_encodings_val_3000.pkl\r\n",
      "descriptions_train_97000_startend.pkl  ixtoword_train_97000.pkl\r\n",
      "descriptions_train_and_val.pkl\t       train_imgs_keys_97000_randomSplit444.pkl\r\n",
      "descriptions_val_3000.pkl\t       val_imgs_keys_3000_randomSplit444.pkl\r\n",
      "embedding_matrix_6758_200.pkl\t       wordtoix_train_97000.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/thesis-imgcap-run2-deterministic-info/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder_Run_3_Wt_ep_1.h5   Decoder_Run_3_Wt_ep_15.h5  Decoder_Run_3_Wt_ep_4.h5\r\n",
      "Decoder_Run_3_Wt_ep_10.h5  Decoder_Run_3_Wt_ep_16.h5  Decoder_Run_3_Wt_ep_5.h5\r\n",
      "Decoder_Run_3_Wt_ep_11.h5  Decoder_Run_3_Wt_ep_17.h5  Decoder_Run_3_Wt_ep_6.h5\r\n",
      "Decoder_Run_3_Wt_ep_12.h5  Decoder_Run_3_Wt_ep_18.h5  Decoder_Run_3_Wt_ep_7.h5\r\n",
      "Decoder_Run_3_Wt_ep_13.h5  Decoder_Run_3_Wt_ep_2.h5   Decoder_Run_3_Wt_ep_8.h5\r\n",
      "Decoder_Run_3_Wt_ep_14.h5  Decoder_Run_3_Wt_ep_3.h5   Decoder_Run_3_Wt_ep_9.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/imgcap-kagg-run3-weights-in/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook_source__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '../working/weights_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook_source__.ipynb  weights_out\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls '../working/weights_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r '../working/weights_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-rBS0Z-E4p8"
   },
   "source": [
    "# Training model on 97k of some 100k images of Coco_Train2017 dataset. Using the balance 3k as the Validation Set. Using all the 5k images of Coco_Val2017 dataset as the Test dataset.\n",
    "\n",
    "## First phase of training already done and pickled various files to resume training with the data being split in a deterministic manner.\n",
    "\n",
    "## Will not do all the steps again and simply load all the info from the pickled files:\n",
    "## Descriptions, image_encodings, embedding_matrix, etc.\n",
    "\n",
    "### The encoder is pre-trained Google Inception-v3 trained on Imagenet.\n",
    "\n",
    "## AVAILABLE data :\n",
    "### Coco_Train2017     = has 118287 images\n",
    "### Coco_Val2017       = has 5000   images\n",
    "### Combined total     = 123287     images\n",
    "\n",
    "## USED data :\n",
    "### Coco_Train2017     = 100000  images\n",
    "### Coco_Val2017       = 5000    images\n",
    "### Combined total     = 105000  images\n",
    "\n",
    "### Using only the first 100k images of Train2017 + all 5k images of Val2017\n",
    "### Thus total data available for training = 100k + 5k = 105k\n",
    "### Details of split of data:\n",
    "### Training   data = 97000 images from Coco_Train2017\n",
    "### Validation data = 3000  images from Coco_Train2017\n",
    "### Test       data = 5000  images from Coco_Val2017\n",
    "\n",
    "### Note: 1) Each image will have multiple captions (up to 5 as some may be discarded)\n",
    "###       2) Not using the Coco_Test2017 dataset at all as it has no annotations json file which has the captions.\n",
    "\n",
    "## Using this LINK: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n",
    "## Image captioning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "buTVBtveEpqr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "#import itertools\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "## import numpy as np\n",
    "## from numpy import array\n",
    "## import pandas as pd\n",
    "## import matplotlib.pyplot as plt\n",
    "## %matplotlib inline\n",
    "## import string\n",
    "## import os\n",
    "## from PIL import Image\n",
    "## import glob\n",
    "## from pickle import dump, load\n",
    "## from time import time\n",
    "## from keras.preprocessing import sequence\n",
    "## from keras.models import Sequential\n",
    "## from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "## from keras.optimizers import Adam, RMSprop\n",
    "## from keras.layers.wrappers import Bidirectional\n",
    "## from keras.layers.merge import add\n",
    "## from keras.applications.inception_v3 import InceptionV3\n",
    "## from keras.preprocessing import image\n",
    "## from keras.models import Model\n",
    "## from keras import Input, layers\n",
    "## from keras import optimizers\n",
    "## from keras.applications.inception_v3 import preprocess_input\n",
    "## from keras.preprocessing.text import Tokenizer\n",
    "## from keras.preprocessing.sequence import pad_sequences\n",
    "## from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebw9HVoxEptM",
    "outputId": "84656bc7-27ad-46cd-e2fe-bab33c5c0032"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0b0aXNyCEd3p"
   },
   "outputs": [],
   "source": [
    "## Kaggle versions\n",
    "\n",
    "## Weights from training till now\n",
    "OPDIR = r'../working/'\n",
    "\n",
    "## New weights to save here\n",
    "OPDIR_WEIGHTS = r'../working/weights_out/'\n",
    "\n",
    "## Images locations\n",
    "IPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\n",
    "IPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\n",
    "\n",
    "## to make deterministic - saved all the variables based on the data used to train during phase 1\n",
    "## ALL SUBSEQUENT PHASES WILL RELOAD FROM THE DATA IN THIS LOCATION\n",
    "IPDIR_DETERMINISTIC = '../input/thesis-imgcap-run2-deterministic-info/'\n",
    "\n",
    "## Weights from previous stage of training\n",
    "IPDIR_WEIGHTS_IN = r'../input/imgcap-kagg-run3-weights-in/'\n",
    "\n",
    "\n",
    "#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\n",
    "\n",
    "## Google drive versions\n",
    "#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n",
    "#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n",
    "#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\n",
    "#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJgRpN5MjJD3"
   },
   "source": [
    "## Data Preparation using Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v8nlMCHYock"
   },
   "outputs": [],
   "source": [
    "## Hereafter, I will try to explain the remaining steps by taking a sample example as follows:\n",
    "## Consider we have 3 images and their 3 corresponding captions as follows:\n",
    "## \n",
    "## (Train image 1) Caption -> The black cat sat on grass\n",
    "## (Train image 2) Caption -> The white cat is walking on road\n",
    "## (Test image) Caption -> The black cat is walking on grass\n",
    "## \n",
    "## Now, let’s say we use the first two images and their captions to train the model and the third image to test our model.\n",
    "## Now the questions that will be answered are: how do we frame this as a supervised learning problem?, what does the data matrix look like? how many data points do we have?, etc.\n",
    "## First we need to convert both the images to their corresponding 2048 length feature vector as discussed above. Let “Image_1” and “Image_2” be the feature vectors of the first two images respectively\n",
    "## Secondly, let’s build the vocabulary for the first two (train) captions by adding the two tokens “startseq” and “endseq” in both of them: (Assume we have already performed the basic cleaning steps)\n",
    "## \n",
    "## Caption_1 -> “startseq the black cat sat on grass endseq”\n",
    "## Caption_2 -> “startseq the white cat is walking on road endseq”\n",
    "## \n",
    "## vocab = {black, cat, endseq, grass, is, on, road, sat, startseq, the, walking, white}\n",
    "## \n",
    "## Let’s give an index to each word in the vocabulary:\n",
    "## black -1, cat -2, endseq -3, grass -4, is -5, on -6, road -7, sat -8, startseq -9, the -10, walking -11, white -12\n",
    "## \n",
    "## Now let’s try to frame it as a supervised learning problem where we have a set of data points D = {Xi, Yi}, where Xi is the feature vector of data point ‘i’ and Yi is the corresponding target variable.\n",
    "## \n",
    "## Let’s take the first image vector Image_1 and its corresponding caption “startseq the black cat sat on grass endseq”. Recall that, Image vector is the input and the caption is what we need to predict. But the way we predict the caption is as follows:\n",
    "## For the first time, we provide the image vector and the first word as input and try to predict the second word, i.e.:\n",
    "## Input = Image_1 + ‘startseq’; Output = ‘the’\n",
    "## Then we provide image vector and the first two words as input and try to predict the third word, i.e.:\n",
    "## Input = Image_1 + ‘startseq the’; Output = ‘cat’\n",
    "## And so on . . .\n",
    "## \n",
    "## Thus, we can summarize the data matrix for one image and its corresponding caption as follows:\n",
    "## Step 1 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq\" :: Target Word = \"the\"\n",
    "## Step 2 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the\" :: Target Word = \"black\"\n",
    "## Step 3 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black\" :: Target Word = \"cat\"\n",
    "## Step 4 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat\" :: Target Word = \"sat\"\n",
    "## Step 5 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat\" :: Target Word = \"on\"\n",
    "## Step 6 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on\" :: Target Word = \"grass\"\n",
    "## Step 7 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on grass\" :: Target Word = \"endseq\"\n",
    "## \n",
    "## It must be noted that, one image+caption is not a single data point but are multiple data points depending on the length of the caption.\n",
    "## All the above 7 data points together constitute the full data for one image and its caption!!!!\n",
    "## Similarly for second images, there will be multiple steps together that consitute its full data point.\n",
    "## \n",
    "## We must now understand that in every data point, it’s not just the image which goes as input to the system, but also, a partial caption which helps to predict the next word in the sequence.\n",
    "## Since we are processing sequences, we will employ a Recurrent Neural Network to read these partial captions (more on this later).\n",
    "## However, we have already discussed that we are not going to pass the actual English text of the caption, rather we are going to pass the sequence of indices where each index represents a unique word.\n",
    "## \n",
    "## Since we have already created an index for each word, let’s now replace the words with their indices and understand how the data matrix will look like:\n",
    "## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"9\" :: Target Word = \"10\"\n",
    "## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10\" :: Target Word = \"1\"\n",
    "## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1\" :: Target Word = \"2\"\n",
    "## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2\" :: Target Word = \"8\"\n",
    "## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8\" :: Target Word = \"6\"\n",
    "## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6\" :: Target Word = \"4\"\n",
    "## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6 4\" :: Target Word = \"3\"\n",
    "## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"9\" :: Target Word = \"10\"\n",
    "## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"9 10\" :: Target Word = \"12\"\n",
    "## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12\" :: Target Word = \"2\"\n",
    "## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2\" :: Target Word = \"5\"\n",
    "## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5\" :: Target Word = \"11\"\n",
    "## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11\" :: Target Word = \"6\"\n",
    "## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6\" :: Target Word = \"7\"\n",
    "## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6 3\" :: Target Word = \"3\"\n",
    "## \n",
    "## Since we would be doing batch processing (explained later), we need to make sure that each sequence is of equal length. Hence we need to append 0’s (zero padding) at the end of each sequence. But how many zeros should we append in each sequence?\n",
    "## Well, this is the reason we had calculated the maximum length of a caption. So we will append those many number of zeros which will lead to every sequence having a length = maximum length of caption.\n",
    "## \n",
    "## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n",
    "## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"1\"\n",
    "## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 0 0 ...]\" :: Target Word = \"2\"\n",
    "## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 0 0 ...]\" :: Target Word = \"8\"\n",
    "## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 0 0 ...]\" :: Target Word = \"6\"\n",
    "## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 0 0 ...]\" :: Target Word = \"4\"\n",
    "## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 4 0 0 ...]\" :: Target Word = \"3\"\n",
    "## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n",
    "## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"12\"\n",
    "## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 0 0 ...]\" :: Target Word = \"2\"\n",
    "## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 0 0 ...]\" :: Target Word = \"5\"\n",
    "## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 0 0 ...]\" :: Target Word = \"11\"\n",
    "## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 0 0 ...]\" :: Target Word = \"6\"\n",
    "## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 0 0 ...]\" :: Target Word = \"7\"\n",
    "## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 3 0 0 ...]\" :: Target Word = \"3\"\n",
    "## Appended adequete 0's to each partial caption to make its length = max length\n",
    "## \n",
    "## \n",
    "## \n",
    "## Need for a Data Generator:\n",
    "## In the above example, I have only considered 2 images and captions which have lead to 15 data points.\n",
    "## However, in our actual training dataset we have 6000 images, each having 5 captions. This makes a total of 30000 images and captions.\n",
    "## Even if we assume that each caption on an average is just 7 words long, it will lead to a total of 30000*7 i.e. 210000 data points.\n",
    "## \n",
    "## Compute the size of the data matrix:\n",
    "## \n",
    "## \n",
    "##    img_vector = 2048        partial caption (length = max caption length)\n",
    "## ---------------------------------------------------------------------------\n",
    "## -                       -                                                 -\n",
    "## -                       -                                                 -\n",
    "## -                       -                                                 -\n",
    "## -                       -                                                 -\n",
    "## ---------------------------------------------------------------------------\n",
    "## Say above matrix has n rows, m columns, so Size of the data matrix = n*m\n",
    "## n-> number of data points (assumed as 210000)  (6000 images * 5 captions per image * 7 words average legnth of each caption)\n",
    "## m-> length of each data point\n",
    "##     = 2048 + length of partial caption (say something)\n",
    "##     = 2048 + something\n",
    "## \n",
    "## Now this \"something\"  NOTE EQUAL to the max length of caption!\n",
    "## Every word (or index) will be mapped (embedded) to higher dimensional space through one of the word embedding techniques.\n",
    "## As we using GloVe-200, each embedding has 200 floats representing each number.\n",
    "## \n",
    "## So with each caption sentence consisting of max length caption size word-indexes, each word represented by 200 dimensional value:\n",
    "##    Assuming max lenght of caption = 45\n",
    "##    means \"something\" = 2048 + ( 45 * 200 ) = 2048 + 9000 = 11048 float values to represent each sentence\n",
    "## \n",
    "## Therefore, size of data matrix = m*n = 210000 * 11048 = xxx float values!!!\n",
    "## Assuming float takes 2 bytes (very conservative), that still means xxx * 2 = xxx GB\n",
    "## \n",
    "## This is pretty huge requirement and even if we are able to manage to load this much data into the RAM, it will make the system very slow.\n",
    "## For this reason we use data generators a lot in Deep Learning. Data Generators are a functionality which is natively implemented in Python. The ImageDataGenerator class provided by the Keras API is nothing but an implementation of generator function in Python.\n",
    "## \n",
    "## So how does using a generator function solve this problem?\n",
    "## If you know the basics of Deep Learning, then you must know that to train a model on a particular dataset, we use some version of Stochastic Gradient Descent (SGD) like Adam, Rmsprop, Adagrad, etc.\n",
    "## With SGD, we do not calculate the loss on the entire data set to update the gradients. Rather in every iteration, we calculate the loss on a batch of data points (typically 64, 128, 256, etc.) to update the gradients.\n",
    "## \n",
    "## This means that we do not require to store the entire dataset in the memory at once. Even if we have the current batch of points in the memory, it is sufficient for our purpose.\n",
    "## A generator function in Python is used exactly for this purpose. It’s like an iterator which resumes the functionality from the point it left the last time it was called.\n",
    "## To understand more about Generators, please read here (https://wiki.python.org/moin/Generators).\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dQkTM-yDjHkU"
   },
   "outputs": [],
   "source": [
    "# data generator, use during the call to model.fit_generator() to create batchwise data\n",
    "def data_generator_1(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch, _vocab_size):\n",
    "    X1, X2, y = [] , [] , []  ## empty lists to populate the input and target data for a bath\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in _descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the encoded features of image\n",
    "            img_feat = _imgs_features_arr[ key ] ## keys in the image encodings dict and descriptions dict use image_filename without the .jpg\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [_wordtoix[word] for word in desc.split(' ') if word in _wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=_max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = keras.utils.to_categorical([out_seq], num_classes=_vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(img_feat)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n == _images_per_batch:\n",
    "                ## ValueError: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0'].\n",
    "                #yield [[np.array(X1), np.array(X2)], np.array(y)]\n",
    "                yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload everything needed from pickled files created based on a certain split of Train and Validation datasets 97k:3k.\n",
    "## Run necessary code again to check all good to proceed with Training phase:\n",
    "## Outcome expected:\n",
    "## 1) unique words in original vocabulary = 24323\n",
    "## 2) unique words in culled vocab vocab_threshold = 6757\n",
    "##    Thus, VOCAB_SIZE = 6757 + 1 = 6758\n",
    "## 3) Embedding matrix shape should be (6758,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../input/thesis-imgcap-run2-deterministic-info/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_DETERMINISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['descriptions_test.pkl',\n",
       " 'img_encodings_train_97000.pkl',\n",
       " 'ixtoword_train_97000.pkl',\n",
       " 'descriptions_val_3000.pkl',\n",
       " 'descriptions_train_97000_startend.pkl',\n",
       " 'descriptions_train_97000.pkl',\n",
       " 'wordtoix_train_97000.pkl',\n",
       " 'embedding_matrix_6758_200.pkl',\n",
       " 'train_imgs_keys_97000_randomSplit444.pkl',\n",
       " 'img_encodings_val_3000.pkl',\n",
       " 'val_imgs_keys_3000_randomSplit444.pkl',\n",
       " 'descriptions_train_and_val.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_DETERMINISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded it all\n"
     ]
    }
   ],
   "source": [
    "## Reload all the files to make it deterministic\n",
    "\n",
    "\n",
    "# variable = img_encodings_train\n",
    "with open(IPDIR_DETERMINISTIC + 'img_encodings_train_97000.pkl', 'rb') as handle:\n",
    "    img_encodings_train = pickle.load(handle)\n",
    "\n",
    "# variable = img_encodings_val\n",
    "with open(IPDIR_DETERMINISTIC + 'img_encodings_val_3000.pkl', 'rb') as handle:\n",
    "    img_encodings_val = pickle.load(handle)\n",
    "\n",
    "# variable = descriptions_train\n",
    "with open(IPDIR_DETERMINISTIC + 'descriptions_train_97000_startend.pkl', 'rb') as handle:\n",
    "    descriptions_train = pickle.load(handle)\n",
    "\n",
    "# variable = descriptions_val\n",
    "with open(IPDIR_DETERMINISTIC + 'descriptions_val_3000.pkl', 'rb') as handle:\n",
    "    descriptions_val = pickle.load(handle)\n",
    "\n",
    "# variable = train_imgs_keys\n",
    "with open(IPDIR_DETERMINISTIC + 'train_imgs_keys_97000_randomSplit444.pkl', 'rb') as handle:\n",
    "    train_imgs_keys = pickle.load(handle)\n",
    "\n",
    "# variable = val_imgs_keys\n",
    "with open(IPDIR_DETERMINISTIC + 'val_imgs_keys_3000_randomSplit444.pkl', 'rb') as handle:\n",
    "    val_imgs_keys = pickle.load(handle)\n",
    "\n",
    "# variable = embedding_matrix\n",
    "with open(IPDIR_DETERMINISTIC + 'embedding_matrix_6758_200.pkl', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)\n",
    "\n",
    "# variable = ixtoword\n",
    "with open(IPDIR_DETERMINISTIC + 'ixtoword_train_97000.pkl', 'rb') as handle:\n",
    "    ixtoword = pickle.load(handle)\n",
    "\n",
    "# variable = wordtoix\n",
    "with open(IPDIR_DETERMINISTIC + 'wordtoix_train_97000.pkl', 'rb') as handle:\n",
    "    wordtoix = pickle.load(handle)\n",
    "\n",
    "print(f\"Reloaded it all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the reloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings data:\n",
      "len(img_encodings_train) = 97000\t\tlen(img_encodings_val) = 3000\n",
      "Descriptions data:\n",
      "len(descriptions_train) = 97000\t\tlen(descriptions_val) = 3000\n",
      "\n",
      "CHECK : reloaded values = 97k for Train , 3k for Validation\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\t\\tlen(img_encodings_val) = {len(img_encodings_val)}\")\n",
    "print(f\"Descriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\t\\tlen(descriptions_val) = {len(descriptions_val)}\")\n",
    "print(f\"\\nCHECK : reloaded values = 97k for Train , 3k for Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size with all words = 24323\n",
      "\n",
      "CHECK : reloaded value = 24323\n"
     ]
    }
   ],
   "source": [
    "## at this stage the descriptions_train already has the start and end tokens added to it\n",
    "vocabulary = set()\n",
    "for key in descriptions_train.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions_train[key]]\n",
    "print(f\"Original Vocabulary Size with all words = {len(vocabulary)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 24323\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culled vocabulary to only retain words occurring more than threshold = 10 times.\n",
      "New vocab size , len(vocab_threshold) = 6757\n",
      "\n",
      "CHECK : reloaded value = 6757\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions, find the freq and retain words where the freq > threshold chosen\n",
    "\n",
    "all_desc_in_training_samples = []\n",
    "for key, val in descriptions_train.items():\n",
    "    for cap in val:\n",
    "        all_desc_in_training_samples.append(cap)\n",
    "\n",
    "MIN_WORD_COUNT_THRESHOLD = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for each_desc in all_desc_in_training_samples:\n",
    "    nsents += 1\n",
    "    for w in each_desc.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab_threshold = [w for w in word_counts if word_counts[w] >= MIN_WORD_COUNT_THRESHOLD]\n",
    "\n",
    "print(f\"Culled vocabulary to only retain words occurring more than threshold = {MIN_WORD_COUNT_THRESHOLD} times.\\nNew vocab size , len(vocab_threshold) = {len(vocab_threshold)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 6757\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Description Length: 49\n",
      "\n",
      "CHECK : reloaded value = 49\n"
     ]
    }
   ],
   "source": [
    "## determine the maximum sequence length - parameter MAX_LENGTH_CAPTION used during the RNN deocder model setup\n",
    "\n",
    "## convert a dictionary of clean descriptions to a list of descriptions\n",
    "def extract_each_desc(_descriptions):\n",
    "    all_desc = list()\n",
    "    for key in _descriptions.keys():\n",
    "        [all_desc.append(d) for d in _descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "## find the longest description length\n",
    "def find_max_length_desc(_descriptions):\n",
    "    desc_sentences = extract_each_desc(_descriptions)\n",
    "    return max(len(d.split()) for d in desc_sentences)\n",
    "\n",
    "MAX_LENGTH_CAPTION = find_max_length_desc(descriptions_train)  ## will be used directly later while defining Decoder model\n",
    "print(f\"Max Description Length: {MAX_LENGTH_CAPTION}\")\n",
    "print(f\"\\nCHECK : reloaded value = 49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(wordtoix) = 6757\n",
      "\n",
      "CHECK : reloaded value = 6757\n",
      "\n",
      "\n",
      "Set the   VOCAB_SIZE = len(wordtoix) + 1 = 6758\n",
      "\n",
      "\n",
      "Set the   EMBEDDING_DIMS = 200\n"
     ]
    }
   ],
   "source": [
    "## the value now, as it will be used as:: VOCAB_SIZE = len(wordtoix) + 1\n",
    "print(f\"len(wordtoix) = {len(wordtoix)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 6757\")\n",
    "\n",
    "VOCAB_SIZE = len(wordtoix) + 1\n",
    "print(f\"\\n\\nSet the   VOCAB_SIZE = len(wordtoix) + 1 = {VOCAB_SIZE}\")\n",
    "\n",
    "EMBEDDING_DIMS = 200\n",
    "print(f\"\\n\\nSet the   EMBEDDING_DIMS = {EMBEDDING_DIMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9 526\n"
     ]
    }
   ],
   "source": [
    "## see the index output by wordtoix for the start and end sequence tokens as well as some random one word\n",
    "print( wordtoix.get('startseq') , wordtoix.get('endseq') , wordtoix.get('cat') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding_matrix = (6758, 200)\n",
      "\n",
      "CHECK : reloaded shape = 6758,200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of embedding_matrix = {embedding_matrix.shape}\")\n",
    "print(f\"\\nCHECK : reloaded shape = 6758,200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD be:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "\n",
      "The values to be used in Decoder setup:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n"
     ]
    }
   ],
   "source": [
    "## Recheck these parameters that will define the Decoder architecture ::: EMBEDDING_DIMS , VOCAB_SIZE , MAX_LENGTH_CAPTION\n",
    "print(f\"SHOULD be:\\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\")\n",
    "print(f\"\\nThe values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTxczXFBRp85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGzQr4tiiWDc"
   },
   "source": [
    "## ALWAYS - Define the RNN Decoder model\n",
    "## Reload ALL weights from previous training point. Then freeze ONLY the embedding layer as before. Thus, the embedding layer will continue to use the GloVe-200 embeddings matrix weights that were first set during Training Phase 1 setup.\n",
    "\n",
    "\n",
    "### About Keras Emebedding layer\n",
    "### \n",
    "#### Default initializing of this layer: https://keras.io/api/layers/core_layers/embedding/\n",
    "######tf.keras.layers.Embedding(\n",
    "######    input_dim, output_dim,\n",
    "######    embeddings_initializer=\"uniform\",\n",
    "######    embeddings_regularizer=None,\n",
    "######    activity_regularizer=None,\n",
    "######    embeddings_constraint=None,\n",
    "######    mask_zero=False,\n",
    "######    input_length=None,\n",
    "######    **kwargs\n",
    "######)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../input/imgcap-kagg-run3-weights-in/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_WEIGHTS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Decoder_Run_3_Wt_ep_9.h5',\n",
       " 'Decoder_Run_3_Wt_ep_5.h5',\n",
       " 'Decoder_Run_3_Wt_ep_12.h5',\n",
       " 'Decoder_Run_3_Wt_ep_7.h5',\n",
       " 'Decoder_Run_3_Wt_ep_6.h5',\n",
       " 'Decoder_Run_3_Wt_ep_11.h5',\n",
       " 'Decoder_Run_3_Wt_ep_8.h5',\n",
       " 'Decoder_Run_3_Wt_ep_3.h5',\n",
       " 'Decoder_Run_3_Wt_ep_14.h5',\n",
       " 'Decoder_Run_3_Wt_ep_2.h5',\n",
       " 'Decoder_Run_3_Wt_ep_16.h5',\n",
       " 'Decoder_Run_3_Wt_ep_18.h5',\n",
       " 'Decoder_Run_3_Wt_ep_4.h5',\n",
       " 'Decoder_Run_3_Wt_ep_1.h5',\n",
       " 'Decoder_Run_3_Wt_ep_17.h5',\n",
       " 'Decoder_Run_3_Wt_ep_15.h5',\n",
       " 'Decoder_Run_3_Wt_ep_13.h5',\n",
       " 'Decoder_Run_3_Wt_ep_10.h5']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_WEIGHTS_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT - SPECIFY CORRECT WEIGHTS FILE TO LOAD FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD_WEIGHTS_FILE_NAME = r'Decoder_Run_3_Wt_ep_18.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will reload this file = ../input/imgcap-kagg-run3-weights-in/Decoder_Run_3_Wt_ep_18.h5\n",
      "Verfiy the weights in file exists : Check value is True = True\n"
     ]
    }
   ],
   "source": [
    "RELOAD_WEIGHTS_FILE_PATH = IPDIR_WEIGHTS_IN + RELOAD_WEIGHTS_FILE_NAME\n",
    "print(f\"Will reload this file = {RELOAD_WEIGHTS_FILE_PATH}\")\n",
    "print(f\"Verfiy the weights in file exists : Check value is True = {os.path.exists(RELOAD_WEIGHTS_FILE_PATH) and os.path.isfile(RELOAD_WEIGHTS_FILE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameter_counts(_model):\n",
    "    total_params = _model.count_params()\n",
    "    total_trainable_params = int(np.sum([np.prod(v.get_shape().as_list()) for v in _model.trainable_variables]))\n",
    "    total_non_trainable_params = int(np.sum([np.prod(v.get_shape().as_list()) for v in _model.non_trainable_variables]))\n",
    "    #print(f\"Total params = {total_decoder_params}\\t Trainable total = {total_decoder_trainable_params}\\t Non-trainable total = {total_decoder_non_trainable_params}\")\n",
    "    return total_params, total_trainable_params, total_non_trainable_params\n",
    "\n",
    "def reload_rnn_encoder_for_more_training(_saved_weights_file, _EMBEDDING_DIMS, _VOCAB_SIZE, _MAX_LENGTH_CAPTION):\n",
    "    if os.path.exists(_saved_weights_file) and os.path.isfile(_saved_weights_file):\n",
    "        ## Decoder Model defining\n",
    "        \n",
    "        ## parameters to define model - sent during function call\n",
    "        #EMBEDDING_DIMS is initialised earlier while creating embedding matrix\n",
    "        #VOCAB_SIZE is initialised earlier\n",
    "        #MAX_LENGTH_CAPTION is initialised earlier\n",
    "        \n",
    "        inputs1 = keras.Input(shape=(2048,))\n",
    "        fe1 = keras.layers.Dropout(0.5)(inputs1)\n",
    "        fe2 = keras.layers.Dense(256, activation='relu')(fe1)\n",
    "        \n",
    "        # partial caption sequence model\n",
    "        inputs2 = keras.Input(shape=(_MAX_LENGTH_CAPTION,))\n",
    "        se1 = keras.layers.Embedding(_VOCAB_SIZE, _EMBEDDING_DIMS, mask_zero=True)(inputs2)\n",
    "        se2 = keras.layers.Dropout(0.5)(se1)\n",
    "        se3 = keras.layers.LSTM(256)(se2)\n",
    "        \n",
    "        # decoder (feed forward) model\n",
    "        decoder1 = keras.layers.add([fe2, se3])\n",
    "        decoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\n",
    "        outputs = keras.layers.Dense(_VOCAB_SIZE, activation='softmax')(decoder2)\n",
    "        \n",
    "        # merge the two input models\n",
    "        reloaded_rnn_decoder_model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        \n",
    "        total_p, trainable_p, non_trainable_p = get_model_parameter_counts(reloaded_rnn_decoder_model)\n",
    "        print(f\"\\nCreated RNN Decoder model defined with these paramenters:\\nEMBEDDING_DIMS = {_EMBEDDING_DIMS} , VOCAB_SIZE = {_VOCAB_SIZE} , MAX_LENGTH_CAPTION = {_MAX_LENGTH_CAPTION}\")\n",
    "        print(f\"\\nBEFORE LOADING WEIGHTS AND FREEZING, parameter counts:\\nTotal parameters = {total_p} , Trainable parameter = {trainable_p} , Non-trainable parameters = {non_trainable_p}\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\nAttempting to load weights...\\n\")\n",
    "        \n",
    "        ## load the weights\n",
    "        reloaded_rnn_decoder_model.load_weights(_saved_weights_file)\n",
    "        print(f\"SUCCESS - Reloaded weights from :: {_saved_weights_file}\")\n",
    "        \n",
    "        ## freeze the embeddings layer weights so they are non-trainable\n",
    "        reloaded_rnn_decoder_model.layers[2].trainable = False\n",
    "        print(f\"\\nFrozen embeddings layer.\")\n",
    "        \n",
    "        total_p, trainable_p, non_trainable_p = get_model_parameter_counts(reloaded_rnn_decoder_model)\n",
    "        print(f\"\\nAFTER FREEZING, parameter counts:\\nTotal parameters = {total_p} , Trainable parameter = {trainable_p} , Non-trainable parameters = {non_trainable_p}\")\n",
    "        \n",
    "        return reloaded_rnn_decoder_model\n",
    "    else:\n",
    "        print(f\"\\nERROR reloading weights. Check weights file exists here = {_saved_weights_file} ;\\nOR model setup parameters incompatible with the saved weights file given.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values to be used in Decoder setup:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "Weights to be reloaded from here:\n",
      "../input/imgcap-kagg-run3-weights-in/Decoder_Run_3_Wt_ep_18.h5\n"
     ]
    }
   ],
   "source": [
    "print(f\"The values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\n",
    "print(f\"Weights to be reloaded from here:\\n{RELOAD_WEIGHTS_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created RNN Decoder model defined with these paramenters:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "\n",
      "BEFORE LOADING WEIGHTS AND FREEZING, parameter counts:\n",
      "Total parameters = 4146710 , Trainable parameter = 4146710 , Non-trainable parameters = 0\n",
      "\n",
      "Attempting to load weights...\n",
      "\n",
      "SUCCESS - Reloaded weights from :: ../input/imgcap-kagg-run3-weights-in/Decoder_Run_3_Wt_ep_18.h5\n",
      "\n",
      "Frozen embeddings layer.\n",
      "\n",
      "AFTER FREEZING, parameter counts:\n",
      "Total parameters = 4146710 , Trainable parameter = 2795110 , Non-trainable parameters = 1351600\n",
      "\n",
      "<class 'tensorflow.python.keras.engine.functional.Functional'>\n"
     ]
    }
   ],
   "source": [
    "reloaded_RNN_decoder = reload_rnn_encoder_for_more_training(RELOAD_WEIGHTS_FILE_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\n",
    "if reloaded_RNN_decoder is None:\n",
    "    print(f\"FATAL ERROR setting up Decoder\")\n",
    "else:\n",
    "    print(f\"\\n{type(reloaded_RNN_decoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "K_mLYJjsRqMO",
    "outputId": "809ebcdd-f972-4f47-f60f-a207404054a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 49, 200)      1351600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 49, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6758)         1736806     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,146,710\n",
      "Trainable params: 2,795,110\n",
      "Non-trainable params: 1,351,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n",
    "## NOTE the count of Trainable params has REDUCED\n",
    "reloaded_RNN_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n",
    "## NOTE the count of Trainable params has REDUCED\n",
    "\n",
    "#Model: \"functional_3\"\n",
    "#__________________________________________________________________________________________________\n",
    "#Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "#==================================================================================================\n",
    "#input_4 (InputLayer)            [(None, 49)]         0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#input_3 (InputLayer)            [(None, 2048)]       0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#embedding_1 (Embedding)         (None, 49, 200)      1351600     input_4[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_2 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_3 (Dropout)             (None, 49, 200)      0           embedding_1[0][0]                \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_3 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#lstm_1 (LSTM)                   (None, 256)          467968      dropout_3[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
    "#                                                                 lstm_1[0][0]                     \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_5 (Dense)                 (None, 6758)         1736806     dense_4[0][0]                    \n",
    "#==================================================================================================\n",
    "#Total params: 4,146,710\n",
    "#Trainable params: 2,795,110\n",
    "#Non-trainable params: 1,351,600\n",
    "#__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(reloaded_RNN_decoder, to_file=OPDIR+'RNN_decoder_model3_plot_1.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNo79p0Az37z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB81Ohfoz4lL"
   },
   "source": [
    "### ALWAYS - Compile RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CF5Dw9brxxwh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default LR without explicity setting so far = 0.0010000000474974513\n",
      "After setting LR as 0.5 = 0.5\n"
     ]
    }
   ],
   "source": [
    "## setup the optimizer and compile\n",
    "\n",
    "## see the default LR used, set some dummy to check it works\n",
    "optimizer_adam = tf.keras.optimizers.Adam()  ## if nothing specified the default LR = 0.001\n",
    "reloaded_RNN_decoder.compile(loss='categorical_crossentropy', optimizer=optimizer_adam)\n",
    "print(f\"Default LR without explicity setting so far = {reloaded_RNN_decoder.optimizer.learning_rate.numpy()}\")\n",
    "\n",
    "optimizer_adam.learning_rate.assign(0.5)\n",
    "print(f\"After setting LR as 0.5 = {reloaded_RNN_decoder.optimizer.learning_rate.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIqp5fxB0ldP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdJW1J2YmrxO"
   },
   "source": [
    "## ALWAYS - Train the RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../working/weights_out/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir( OPDIR_WEIGHTS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EXJ2Yjuz05wG",
    "outputId": "c0ab13d8-b8d2-4450-ef87-589ebd49b46c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEncodings data:\n",
      "len(img_encodings_train) = 97000\n",
      "len(img_encodings_val) = 3000\n",
      "\tDescriptions data:\n",
      "len(descriptions_train) = 97000\n",
      "len(descriptions_val) = 3000\n",
      "\n",
      "NO Validation Set being used for now.\n",
      "\n",
      "len(wordtoix) = 6757\n",
      "VOCAB_SIZE = 6758\n",
      "EMBEDDING_DIMS = 200\n",
      "embedding_matrix Shape = (6758, 200)\n",
      "MAX_LENGTH_CAPTION = 49\n",
      "\n",
      "Initialed with this input weights file:\n",
      "../input/imgcap-kagg-run3-weights-in/Decoder_Run_3_Wt_ep_18.h5\n"
     ]
    }
   ],
   "source": [
    "## values setup earlier - for reference\n",
    "\n",
    "print(f\"\\tEncodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\nlen(img_encodings_val) = {len(img_encodings_val)}\")\n",
    "print(f\"\\tDescriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\nlen(descriptions_val) = {len(descriptions_val)}\")\n",
    "\n",
    "print(f\"\\nNO Validation Set being used for now.\\n\")\n",
    "\n",
    "print(f\"len(wordtoix) = {len(wordtoix)}\")\n",
    "print(f\"VOCAB_SIZE = {VOCAB_SIZE}\")\n",
    "print(f\"EMBEDDING_DIMS = {EMBEDDING_DIMS}\")\n",
    "print(f\"embedding_matrix Shape = {embedding_matrix.shape}\")\n",
    "print(f\"MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\n",
    "\n",
    "print(f\"\\nInitialed with this input weights file:\\n{RELOAD_WEIGHTS_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_RUN_NUMBER = 3 ## large training data - 97k train\n",
    "MODEL_RUN_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Specify the last epoch number (index from 1, NOT 0) so that the output weights files will have the correct epoch number for tracking\n",
    "## E.g. if earlier phase ran up to Ep 7, then specify below as 7\n",
    "EARLIER_TRAINING_EPOCH_END = 18\n",
    "EARLIER_TRAINING_EPOCH_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MODEL 3 :: Training Phase 10 started at :: 18:33:37\n",
      "\n",
      "\n",
      "Phase 10 parameters:\n",
      "STEPS_PER_EPOCH_10 = 6062\n",
      "BATCH_SIZE_10 = 16\n",
      "N_EPOCHS_10 = 2\n",
      "EARLIER_TRAINING_EPOCH_END = 18\n",
      "\n",
      "Epoch 19 started at 18:33:37\n",
      "LR used = 0.0005000000237487257\n",
      "\n",
      " 297/6062 [>.............................] - ETA: 1:46:23 - loss: 2.7401"
     ]
    }
   ],
   "source": [
    "########### Phase 10 #########\n",
    "\n",
    "print(f\"\\n\\nMODEL {MODEL_RUN_NUMBER} :: Training Phase 10 started at :: {datetime.datetime.now().strftime('%H:%M:%S')}\\n\")\n",
    "\n",
    "LR_10 = 0.0005\n",
    "BATCH_SIZE_10 = 16   ## how many images per batch\n",
    "N_EPOCHS_10 = 2\n",
    "STEPS_PER_EPOCH_10 = len(descriptions_train) // BATCH_SIZE_10\n",
    "print(f\"\\nPhase 10 parameters:\")\n",
    "print(f\"STEPS_PER_EPOCH_10 = {STEPS_PER_EPOCH_10}\")\n",
    "print(f\"BATCH_SIZE_10 = {BATCH_SIZE_10}\")\n",
    "print(f\"N_EPOCHS_10 = {N_EPOCHS_10}\")\n",
    "print(f\"EARLIER_TRAINING_EPOCH_END = {EARLIER_TRAINING_EPOCH_END}\")\n",
    "\n",
    "optimizer_adam.learning_rate.assign(LR_10)\n",
    "\n",
    "for i in range( EARLIER_TRAINING_EPOCH_END , EARLIER_TRAINING_EPOCH_END + N_EPOCHS_10 ):\n",
    "    print(f\"\\nEpoch {i+1} started at {datetime.datetime.now().strftime('%H:%M:%S')}\\nLR used = {reloaded_RNN_decoder.optimizer.learning_rate.numpy()}\\n\")\n",
    "    generator_1 = data_generator_1(descriptions_train, img_encodings_train, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_10, VOCAB_SIZE)\n",
    "    reloaded_RNN_decoder.fit_generator(generator_1, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_10, verbose=1)\n",
    "    \n",
    "    reloaded_RNN_decoder.save_weights( OPDIR_WEIGHTS + 'Decoder_Run_' + str(MODEL_RUN_NUMBER) + '_Wt_ep_' + str(i+1) + '.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Phase 10 #########   --- NOTE changed BS = 16 from 32\n",
    "\n",
    "MODEL 3 :: Training Phase 10 started at :: 18:33:37\n",
    "\n",
    "\n",
    "Phase 10 parameters:\n",
    "STEPS_PER_EPOCH_10 = 6062\n",
    "BATCH_SIZE_10 = 16\n",
    "N_EPOCHS_10 = 2\n",
    "EARLIER_TRAINING_EPOCH_END = 18\n",
    "\n",
    "Epoch 19 started at 18:33:37\n",
    "LR used = 0.0005000000237487257\n",
    "\n",
    " 292/6062 [>.............................] - ETA: 1:46:33 - loss: 2.7403\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEEP TRACK OF EARLIER PHASE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let it fail if run accidentally\n",
    "\n",
    "COMMON stuff\n",
    "\n",
    "        Encodings data:\n",
    "len(img_encodings_train) = 97000\n",
    "len(img_encodings_val) = 3000\n",
    "        Descriptions data:\n",
    "len(descriptions_train) = 97000\n",
    "len(descriptions_val) = 3000\n",
    "\n",
    "No Validation Set being used for now.\n",
    "\n",
    "len(wordtoix) = 6757\n",
    "VOCAB_SIZE = 6758\n",
    "EMBEDDING_DIMS = 200\n",
    "embedding_matrix Shape = (6758, 200)\n",
    "MAX_LENGTH_CAPTION = 49\n",
    "\n",
    "                Before freezing:\n",
    "#Total params: 4,146,710\n",
    "#Trainable params: 4,146,710\n",
    "#Non-trainable params: 0\n",
    "\n",
    "                After freezing:\n",
    "#Total params: 4,146,710\n",
    "#Trainable params: 2,795,110\n",
    "#Non-trainable params: 1,351,600\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "Phase 1:  -- had set to run 3 epochs but only epoch 1/3 weight file avaliable.\n",
    "\n",
    "LR_1 = 0.001\n",
    "BATCH_SIZE_1 = 64   ## how many images per batch\n",
    "N_EPOCHS_1 = 3\n",
    "Adam optimizer\n",
    "\n",
    "\n",
    "MODEL 2 :: Training Phase 1 started at :: 22:44:01\n",
    "\n",
    "\n",
    "Phase 1 parameters:\n",
    "LR_1 = 0.001\n",
    "STEPS_PER_EPOCH_1 = 1515\n",
    "BATCH_SIZE_1 = 64\n",
    "N_EPOCHS_1 = 3\n",
    "\n",
    "Epoch 1 started at 22:44:01\n",
    "LR used = 0.0010000000474974513 \n",
    "\n",
    "1515/1515 [==============================] - 7900s 5s/step - loss: 3.7964\t\t\t\tonly able to save this weights file\n",
    "\n",
    "Epoch 2 started at 00:55:53\n",
    "LR used = 0.0010000000474974513 \n",
    "\n",
    "1515/1515 [==============================] - 7890s 5s/step - loss: 3.1811\n",
    "\n",
    "Epoch 3 started at 03:07:29\n",
    "LR used = 0.0010000000474974513 \n",
    "\n",
    "1515/1515 [==============================] - 7911s 5s/step - loss: 3.0520\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "Phase 2:  -- had set to run 4 epochs but only epoch till epoch 2/4 weight file avaliable.\n",
    "\n",
    "Phase 2 parameters:\n",
    "STEPS_PER_EPOCH_2 = 1515\n",
    "BATCH_SIZE_2 = 64\n",
    "N_EPOCHS_2 = 4\n",
    "\n",
    "Epoch 2 started at 11:59:50\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 6942s 5s/step - loss: 3.1985\n",
    "\n",
    "Epoch 3 started at 13:55:42\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7026s 5s/step - loss: 3.0458\n",
    "\n",
    "Epoch 4 started at 15:52:52\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    " 643/1515 [===========>..................] - ETA: 1:07:01 - loss: 2.9839\t\t\t\tfailed midway\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "Phase 3:   -- had set to run 4 epochs but only epoch till epoch 2/4 weight file avaliable.\n",
    "\n",
    "MODEL 3 :: Training Phase 3 started at :: 19:28:15\n",
    "\n",
    "\n",
    "Phase 3 parameters:\n",
    "STEPS_PER_EPOCH_3 = 1515\n",
    "BATCH_SIZE_3 = 64\n",
    "N_EPOCHS_3 = 4\n",
    "\n",
    "Epoch 4 started at 19:28:15\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7071s 5s/step - loss: 3.0032\n",
    "\n",
    "Epoch 5 started at 21:26:17\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7088s 5s/step - loss: 2.9399\n",
    "\n",
    "Epoch 6 started at 23:24:30\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    " 163/1515 [==>...........................] - ETA: 1:44:44 - loss: 2.8966              FAILED MIDWAY - ONLY TILL EP 5 WEIGHTS SAVED\n",
    "\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "Phase 4:   -- had set to run 3, seems to completed properly with all weight files saved. Only prob is showed a Failed to Fetch message during last ep, but continued runninng. Ignoring the problem\n",
    "\n",
    "MODEL 3 :: Training Phase 4 started at :: 00:10:53\n",
    "\n",
    "\n",
    "Phase 4 parameters:\n",
    "STEPS_PER_EPOCH_4 = 1515\n",
    "BATCH_SIZE_4 = 64\n",
    "N_EPOCHS_4 = 3\n",
    "EARLIER_TRAINING_EPOCH_END = 5\n",
    "\n",
    "Epoch 6 started at 00:10:53\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7917s 5s/step - loss: 2.9315\n",
    "\n",
    "Epoch 7 started at 02:23:01\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7941s 5s/step - loss: 2.8899\n",
    "\n",
    "Epoch 8 started at 04:35:28\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7963s 5s/step - loss: 2.8673\n",
    "\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "Phase 5:  -- had set to run 3 epochs - ALL ran successfully\n",
    "\n",
    "MODEL 3 :: Training Phase 5 started at :: 09:07:50\n",
    "\n",
    "\n",
    "Phase 5 parameters:\n",
    "STEPS_PER_EPOCH_5 = 1515\n",
    "BATCH_SIZE_5 = 64\n",
    "N_EPOCHS_5 = 3\n",
    "EARLIER_TRAINING_EPOCH_END = 8\n",
    "\n",
    "Epoch 9 started at 09:07:50\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7870s 5s/step - loss: 2.8728\n",
    "\n",
    "Epoch 10 started at 11:19:13\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7869s 5s/step - loss: 2.8443\n",
    "\n",
    "Epoch 11 started at 13:30:28\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    " 741/1515 [=============>................] - ETA: 1:07:06 - loss: 2.8187\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "########### Phase 6 #########\n",
    "\n",
    "MODEL 3 :: Training Phase 6 started at :: 18:01:53\n",
    "\n",
    "\n",
    "Phase 6 parameters:\n",
    "STEPS_PER_EPOCH_6 = 1515\n",
    "BATCH_SIZE_6 = 64\n",
    "N_EPOCHS_6 = 4\n",
    "EARLIER_TRAINING_EPOCH_END = 11\n",
    "\n",
    "Epoch 12 started at 18:01:53\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7179s 5s/step - loss: 2.8397\n",
    "\n",
    "Epoch 13 started at 20:01:42\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7219s 5s/step - loss: 2.8169      --- only downloaded till here\n",
    "\n",
    "Epoch 14 started at 22:02:06\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7190s 5s/step - loss: 2.8059\n",
    "\n",
    "Epoch 15 started at 00:02:01\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "1515/1515 [==============================] - 7146s 5s/step - loss: 2.7968\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "\n",
    "########### Phase 7 #########   --- NOTE changed BS = 128 from 64\n",
    "\n",
    "MODEL 3 :: Training Phase 7 started at :: 04:01:59\n",
    "\n",
    "\n",
    "Phase 7 parameters:\n",
    "STEPS_PER_EPOCH_7 = 757\n",
    "BATCH_SIZE_7 = 128\n",
    "N_EPOCHS_7 = 2\n",
    "EARLIER_TRAINING_EPOCH_END = 13\n",
    "\n",
    "Epoch 14 started at 04:01:59\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "757/757 [==============================] - 8378s 11s/step - loss: 2.8013\n",
    "\n",
    "Epoch 15 started at 06:22:00\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "267/757 [=========>....................] - ETA: 1:29:49 - loss: 2.7637        --- failed midway\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "########### Phase 8 #########   --- NOTE changed BS = 128 from 64\n",
    "\n",
    "MODEL 3 :: Training Phase 8 started at :: 08:33:07\n",
    "\n",
    "\n",
    "Phase 8 parameters:\n",
    "STEPS_PER_EPOCH_8 = 757\n",
    "BATCH_SIZE_8 = 128\n",
    "N_EPOCHS_8 = 1\n",
    "EARLIER_TRAINING_EPOCH_END = 14\n",
    "\n",
    "Epoch 15 started at 08:33:07\n",
    "LR used = 0.0010000000474974513\n",
    "\n",
    "757/757 [==============================] - 8268s 11s/step - loss: 2.7950\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "\n",
    "########### Phase 8 #########   --- NOTE changed BS = 32 from 64\n",
    "\n",
    "MODEL 3 :: Training Phase 9 started at :: 11:12:47\n",
    "\n",
    "\n",
    "Phase 9 parameters:\n",
    "STEPS_PER_EPOCH_9 = 3031\n",
    "BATCH_SIZE_9 = 32\n",
    "N_EPOCHS_9 = 3\n",
    "EARLIER_TRAINING_EPOCH_END = 15\n",
    "\n",
    "Epoch 16 started at 11:12:47\n",
    "LR used = 0.0005000000237487257\n",
    "\n",
    "3031/3031 [==============================] - 7951s 3s/step - loss: 2.7977\n",
    "\n",
    "Epoch 17 started at 13:25:26\n",
    "LR used = 0.0005000000237487257\n",
    "\n",
    "3031/3031 [==============================] - 8119s 3s/step - loss: 2.7796\n",
    "\n",
    "Epoch 18 started at 15:40:48\n",
    "LR used = 0.0005000000237487257\n",
    "\n",
    "3031/3031 [==============================] - 8965s 3s/step - loss: 2.7704\n",
    "\n",
    "\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n",
    "************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Xc08wQoRqZx"
   },
   "outputs": [],
   "source": [
    "#model_reset_states\n",
    "#lr optimizer\n",
    "#tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D3b7izSIqUb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
