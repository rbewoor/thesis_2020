{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-rBS0Z-E4p8"
   },
   "source": [
    "# Compare the model caption inference against the ground truth captions from annotations file.\n",
    "# Trained model on 97k of some 100k images of Coco_Train2017 dataset. Balance 3k kept aside as the Validation Set. Using all the 5k images of Coco_Val2017 dataset as the Test dataset.\n",
    "\n",
    "## Encoder: Pre-trained Google Inception-v3 trained on Imagenet imported directly from Keras\n",
    "## Decoder: Trained with the 97k data and their captions from annotations file.\n",
    "\n",
    "## AVAILABLE data :\n",
    "### Coco_Train2017     = has 118287 images\n",
    "### Coco_Val2017       = has 5000   images\n",
    "### Combined total     = 123287     images\n",
    "\n",
    "## USED data :\n",
    "### Coco_Train2017     = 100000  images\n",
    "### Coco_Val2017       = 5000    images\n",
    "### Combined total     = 105000  images\n",
    "\n",
    "### Using only the first 100k images of Train2017 + all 5k images of Val2017\n",
    "### Thus total data available for training = 100k + 5k = 105k\n",
    "### Details of split of data:\n",
    "### Training   data = 97000 images from Coco_Train2017\n",
    "### Validation data = 3000  images from Coco_Train2017\n",
    "### Test       data = 5000  images from Coco_Val2017\n",
    "\n",
    "### Note: 1) Each image will have multiple captions (up to 5 as some may be discarded)\n",
    "###       2) Not using the Coco_Test2017 dataset at all as it has no annotations json file which has the captions.\n",
    "\n",
    "## Using this LINK: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n",
    "## Image captioning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "buTVBtveEpqr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "#import itertools\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "## import numpy as np\n",
    "## from numpy import array\n",
    "## import pandas as pd\n",
    "## import matplotlib.pyplot as plt\n",
    "## %matplotlib inline\n",
    "## import string\n",
    "## import os\n",
    "## from PIL import Image\n",
    "## import glob\n",
    "## from pickle import dump, load\n",
    "## from time import time\n",
    "## from keras.preprocessing import sequence\n",
    "## from keras.models import Sequential\n",
    "## from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "## from keras.optimizers import Adam, RMSprop\n",
    "## from keras.layers.wrappers import Bidirectional\n",
    "## from keras.layers.merge import add\n",
    "## from keras.applications.inception_v3 import InceptionV3\n",
    "## from keras.preprocessing import image\n",
    "## from keras.models import Model\n",
    "## from keras import Input, layers\n",
    "## from keras import optimizers\n",
    "## from keras.applications.inception_v3 import preprocess_input\n",
    "## from keras.preprocessing.text import Tokenizer\n",
    "## from keras.preprocessing.sequence import pad_sequences\n",
    "## from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebw9HVoxEptM",
    "outputId": "84656bc7-27ad-46cd-e2fe-bab33c5c0032"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0b0aXNyCEd3p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## Kaggle versions\\n\\n## Weights from training till now\\nOPDIR = r'../working/'\\n\\n## Images locations\\nIPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\\nIPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\\n\\n## Annotations json file location from where to pick up the captions\\nIPDIR_ANNO = r'../input/coco-2017-dataset/coco2017/annotations/'\\n\\n## Bottleneck CNN Encoder output for all the images to be used in training\\nIPDIR_IMG_ENCODINGS = r'../input/thesis-imgcap-imgencodings-1/'\\n\\n## to make deterministic - saved all the variables based on the data used to train during phase 1\\n## ALL SUBSEQUENT PHASES WILL RELOAD FROM THE DATA IN THIS LOCATION\\nIPDIR_DETERMINISTIC = '../input/thesis-imgcap-run2-deterministic-info/'\\n\\n## Weights from previous stage of training\\nIPDIR_WEIGHTS_IN = r'../input/imgcap-kagg-run2-weights-in/'\\n\\n\\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\\n\\n## Google drive versions\\n#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\\n#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\\n#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LAPTOP versions\n",
    "\n",
    "## Weights from training till now\n",
    "#OPDIR = r'../working/'\n",
    "\n",
    "## Images locations\n",
    "#IPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\n",
    "#IPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\n",
    "\n",
    "## Annotations json file location from where to pick up the captions\n",
    "IPDIR_ANNO = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/coco_2017_annotations/'\n",
    "\n",
    "## Bottleneck CNN Encoder output for all the images of TRAIN , VALIDATION and TEST DATASETS\n",
    "IPDIR_IMG_ENCODINGS = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_ImgEncodings_1/'\n",
    "\n",
    "## to make deterministic - saved all the variables based on the data used to train during phase 1\n",
    "## ALL SUBSEQUENT PHASES WILL RELOAD FROM THE DATA IN THIS LOCATION\n",
    "IPDIR_DETERMINISTIC = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Run2_Deterministic_Info/'\n",
    "\n",
    "## Weights from previous stage of training\n",
    "## run2\n",
    "#IPDIR_WEIGHTS_IN = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run2/'\n",
    "## run3\n",
    "IPDIR_WEIGHTS_IN = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run3/'\n",
    "\n",
    "\"\"\"\n",
    "## Kaggle versions\n",
    "\n",
    "## Weights from training till now\n",
    "OPDIR = r'../working/'\n",
    "\n",
    "## Images locations\n",
    "IPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\n",
    "IPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\n",
    "\n",
    "## Annotations json file location from where to pick up the captions\n",
    "IPDIR_ANNO = r'../input/coco-2017-dataset/coco2017/annotations/'\n",
    "\n",
    "## Bottleneck CNN Encoder output for all the images to be used in training\n",
    "IPDIR_IMG_ENCODINGS = r'../input/thesis-imgcap-imgencodings-1/'\n",
    "\n",
    "## to make deterministic - saved all the variables based on the data used to train during phase 1\n",
    "## ALL SUBSEQUENT PHASES WILL RELOAD FROM THE DATA IN THIS LOCATION\n",
    "IPDIR_DETERMINISTIC = '../input/thesis-imgcap-run2-deterministic-info/'\n",
    "\n",
    "## Weights from previous stage of training\n",
    "IPDIR_WEIGHTS_IN = r'../input/imgcap-kagg-run2-weights-in/'\n",
    "\n",
    "\n",
    "#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\n",
    "\n",
    "## Google drive versions\n",
    "#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n",
    "#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n",
    "#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\n",
    "#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload everything needed from pickled files created based on a certain split of Train and Validation datasets 97k:3k.\n",
    "## Run necessary code again to check all good to proceed with Training phase:\n",
    "## Outcome expected:\n",
    "## 1) unique words in original vocabulary = 24323\n",
    "## 2) unique words in culled vocab vocab_threshold = 6757\n",
    "##    Thus, VOCAB_SIZE = 6757 + 1 = 6758\n",
    "## 3) Embedding matrix shape should be (6758,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Run2_Deterministic_Info/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_DETERMINISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['descriptions_test.pkl',\n",
       " 'descriptions_train_97000.pkl',\n",
       " 'descriptions_train_97000_startend.pkl',\n",
       " 'descriptions_train_and_val.pkl',\n",
       " 'descriptions_val_3000.pkl',\n",
       " 'embedding_matrix_6758_200.pkl',\n",
       " 'img_encodings_train_97000.pkl',\n",
       " 'img_encodings_val_3000.pkl',\n",
       " 'ixtoword_train_97000.pkl',\n",
       " 'train_imgs_keys_97000_randomSplit444.pkl',\n",
       " 'val_imgs_keys_3000_randomSplit444.pkl',\n",
       " 'wordtoix_train_97000.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_DETERMINISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded it all\n"
     ]
    }
   ],
   "source": [
    "## Reload all the files to make it deterministic\n",
    "\n",
    "\n",
    "# variable = img_encodings_train\n",
    "with open(IPDIR_DETERMINISTIC + 'img_encodings_train_97000.pkl', 'rb') as handle:\n",
    "    img_encodings_train = pickle.load(handle)\n",
    "\n",
    "# variable = img_encodings_val\n",
    "with open(IPDIR_DETERMINISTIC + 'img_encodings_val_3000.pkl', 'rb') as handle:\n",
    "    img_encodings_val = pickle.load(handle)\n",
    "\n",
    "# variable = descriptions_train\n",
    "with open(IPDIR_DETERMINISTIC + 'descriptions_train_97000_startend.pkl', 'rb') as handle:\n",
    "    descriptions_train = pickle.load(handle)\n",
    "\n",
    "# variable = descriptions_val\n",
    "with open(IPDIR_DETERMINISTIC + 'descriptions_val_3000.pkl', 'rb') as handle:\n",
    "    descriptions_val = pickle.load(handle)\n",
    "\n",
    "# variable = train_imgs_keys\n",
    "with open(IPDIR_DETERMINISTIC + 'train_imgs_keys_97000_randomSplit444.pkl', 'rb') as handle:\n",
    "    train_imgs_keys = pickle.load(handle)\n",
    "\n",
    "# variable = val_imgs_keys\n",
    "with open(IPDIR_DETERMINISTIC + 'val_imgs_keys_3000_randomSplit444.pkl', 'rb') as handle:\n",
    "    val_imgs_keys = pickle.load(handle)\n",
    "\n",
    "# variable = embedding_matrix\n",
    "with open(IPDIR_DETERMINISTIC + 'embedding_matrix_6758_200.pkl', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)\n",
    "\n",
    "# variable = ixtoword\n",
    "with open(IPDIR_DETERMINISTIC + 'ixtoword_train_97000.pkl', 'rb') as handle:\n",
    "    ixtoword = pickle.load(handle)\n",
    "\n",
    "# variable = wordtoix\n",
    "with open(IPDIR_DETERMINISTIC + 'wordtoix_train_97000.pkl', 'rb') as handle:\n",
    "    wordtoix = pickle.load(handle)\n",
    "\n",
    "print(f\"Reloaded it all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the reloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings data:\n",
      "len(img_encodings_train) = 97000\t\tlen(img_encodings_val) = 3000\n",
      "Descriptions data:\n",
      "len(descriptions_train) = 97000\t\tlen(descriptions_val) = 3000\n",
      "\n",
      "CHECK : reloaded values = 97k for Train , 3k for Validation\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\t\\tlen(img_encodings_val) = {len(img_encodings_val)}\")\n",
    "print(f\"Descriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\t\\tlen(descriptions_val) = {len(descriptions_val)}\")\n",
    "print(f\"\\nCHECK : reloaded values = 97k for Train , 3k for Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size with all words = 24323\n",
      "\n",
      "CHECK : reloaded value = 24323\n"
     ]
    }
   ],
   "source": [
    "## at this stage the descriptions_train already has the start and end tokens added to it\n",
    "vocabulary = set()\n",
    "for key in descriptions_train.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions_train[key]]\n",
    "print(f\"Original Vocabulary Size with all words = {len(vocabulary)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 24323\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culled vocabulary to only retain words occurring more than threshold = 10 times.\n",
      "New vocab size , len(vocab_threshold) = 6757\n",
      "\n",
      "CHECK : reloaded value = 6757\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions, find the freq and retain words where the freq > threshold chosen\n",
    "\n",
    "all_desc_in_training_samples = []\n",
    "for key, val in descriptions_train.items():\n",
    "    for cap in val:\n",
    "        all_desc_in_training_samples.append(cap)\n",
    "\n",
    "MIN_WORD_COUNT_THRESHOLD = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for each_desc in all_desc_in_training_samples:\n",
    "    nsents += 1\n",
    "    for w in each_desc.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab_threshold = [w for w in word_counts if word_counts[w] >= MIN_WORD_COUNT_THRESHOLD]\n",
    "\n",
    "print(f\"Culled vocabulary to only retain words occurring more than threshold = {MIN_WORD_COUNT_THRESHOLD} times.\\nNew vocab size , len(vocab_threshold) = {len(vocab_threshold)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 6757\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Description Length: 49\n",
      "\n",
      "CHECK : reloaded value = 49\n"
     ]
    }
   ],
   "source": [
    "## determine the maximum sequence length - parameter MAX_LENGTH_CAPTION used during the RNN deocder model setup\n",
    "\n",
    "## convert a dictionary of clean descriptions to a list of descriptions\n",
    "def extract_each_desc(_descriptions):\n",
    "    all_desc = list()\n",
    "    for key in _descriptions.keys():\n",
    "        [all_desc.append(d) for d in _descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "## find the longest description length\n",
    "def find_max_length_desc(_descriptions):\n",
    "    desc_sentences = extract_each_desc(_descriptions)\n",
    "    return max(len(d.split()) for d in desc_sentences)\n",
    "\n",
    "MAX_LENGTH_CAPTION = find_max_length_desc(descriptions_train)  ## will be used directly later while defining Decoder model\n",
    "print(f\"Max Description Length: {MAX_LENGTH_CAPTION}\")\n",
    "print(f\"\\nCHECK : reloaded value = 49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(wordtoix) = 6757\n",
      "\n",
      "CHECK : reloaded value = 6757\n",
      "\n",
      "\n",
      "Set the   VOCAB_SIZE = len(wordtoix) + 1 = 6758\n",
      "\n",
      "\n",
      "Set the   EMBEDDING_DIMS = 200\n"
     ]
    }
   ],
   "source": [
    "## the value now, as it will be used as:: VOCAB_SIZE = len(wordtoix) + 1\n",
    "print(f\"len(wordtoix) = {len(wordtoix)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 6757\")\n",
    "\n",
    "VOCAB_SIZE = len(wordtoix) + 1\n",
    "print(f\"\\n\\nSet the   VOCAB_SIZE = len(wordtoix) + 1 = {VOCAB_SIZE}\")\n",
    "\n",
    "EMBEDDING_DIMS = 200\n",
    "print(f\"\\n\\nSet the   EMBEDDING_DIMS = {EMBEDDING_DIMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9 526\n"
     ]
    }
   ],
   "source": [
    "## see the index output by wordtoix for the start and end sequence tokens as well as some random one word\n",
    "print( wordtoix.get('startseq') , wordtoix.get('endseq') , wordtoix.get('cat') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding_matrix = (6758, 200)\n",
      "\n",
      "CHECK : reloaded shape = 6758,200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of embedding_matrix = {embedding_matrix.shape}\")\n",
    "print(f\"\\nCHECK : reloaded shape = 6758,200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD be:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "\n",
      "The values to be used in Decoder setup:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n"
     ]
    }
   ],
   "source": [
    "## Recheck these parameters that will define the Decoder architecture ::: EMBEDDING_DIMS , VOCAB_SIZE , MAX_LENGTH_CAPTION\n",
    "print(f\"SHOULD be:\\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\")\n",
    "print(f\"\\nThe values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTxczXFBRp85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the Coco_Val_2017 5k image encodings from pickled file.\n",
    "### NOTE: This is the TEST dataset I am using which has the ground truth captions for comparison against my models inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_ImgEncodings_1/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_IMG_ENCODINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train2017_subset_5k_45000_50000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_20000_25000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_25000_30000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_30000_35000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_35000_40000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_40000_45000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_50000_55000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_55000_60000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_60000_65000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_65000_70000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_70000_75000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_75000_80000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_80000_85000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_85000_90000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_90000_95000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_95000_100000_images_encoded_features_pickled_1.pkl',\n",
       " 'val2017_all_5k_images_encoded_features_pickled_2.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_IMG_ENCODINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading encodings for the test images\n",
      "Loaded file num 1 :: val2017_all_5k_images_encoded_features_pickled_2.pkl\n",
      "Number of entries in img_encodings_test = 5000\n",
      "\n",
      "Final count img_encodings_test = 5000. Should be = 5k.\n"
     ]
    }
   ],
   "source": [
    "## From Val2017 images pickled files\n",
    "print(f\"\\n\\nLoading encodings for the test images\")\n",
    "pickled_encodings_files_val2017 = ['val2017_all_5k_images_encoded_features_pickled_2.pkl']\n",
    "img_encodings_test = {}\n",
    "for idx, subset_encodings_file_val2017 in enumerate(pickled_encodings_files_val2017):\n",
    "    with open(IPDIR_IMG_ENCODINGS + subset_encodings_file_val2017, 'rb') as handle:\n",
    "        img_encodings_test.update(pickle.load(handle))\n",
    "        print(f\"Loaded file num {idx+1} :: {subset_encodings_file_val2017}\")\n",
    "    print(f\"Number of entries in img_encodings_test = {len(img_encodings_test)}\")\n",
    "print(f\"\\nFinal count img_encodings_test = {len(img_encodings_test)}. Should be = 5k.\")\n",
    "\n",
    "del pickled_encodings_files_val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thus \"img_encodings_test\" has the CNN encoder output feature encodings for all 5k images\n",
    "## All these values in the dict is a numpy array of shape (2048,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img_encodings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000179765': array([0.14290808, 0.14481388, 0.30199888, ..., 0.20583029, 0.1378399 ,\n",
       "        0.05842396], dtype=float32),\n",
       " '000000190236': array([0.13693401, 0.44517994, 0.8012962 , ..., 0.09662007, 0.5428594 ,\n",
       "        0.12551737], dtype=float32),\n",
       " '000000331352': array([0.36297214, 0.09468681, 0.14937937, ..., 0.03184601, 0.68944204,\n",
       "        0.49036297], dtype=float32),\n",
       " '000000517069': array([0.16364594, 0.08695374, 0.61649114, ..., 0.74132663, 0.09887794,\n",
       "        0.5194288 ], dtype=float32),\n",
       " '000000182417': array([0.32260168, 0.7291188 , 1.62285   , ..., 0.31113395, 0.7642602 ,\n",
       "        0.21318905], dtype=float32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(img_encodings_test.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img_encodings_test['000000179765'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14290808, 0.14481388, 0.30199888, ..., 0.20583029, 0.1378399 ,\n",
       "       0.05842396], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_encodings_test['000000179765']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_encodings_test['000000179765'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the annotations file to load the descriptions and clean as per standard processing followed overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/coco_2017_annotations/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_ANNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['captions_train2017.json', 'captions_val2017.json']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_ANNO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179765</td>\n",
       "      <td>38</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179765</td>\n",
       "      <td>182</td>\n",
       "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190236</td>\n",
       "      <td>401</td>\n",
       "      <td>An office cubicle with four different types of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id   id                                            caption\n",
       "0    179765   38  A black Honda motorcycle parked in front of a ...\n",
       "1    179765  182      A Honda motorcycle parked in a grass driveway\n",
       "2    190236  401  An office cubicle with four different types of..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the descriptions_test_dataset dict\n",
    "## BUT NOTE: it is actually from the captions_val2017 dataset - which I treat as my TEST DATASET\n",
    "\n",
    "with open(IPDIR_ANNO+'captions_val2017.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n",
    "  #type(data['annotations']) # is a list\n",
    "  #type(data['images'])      # also is a list\n",
    "\n",
    "dfanno = pd.DataFrame(data=data['annotations'])\n",
    "# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n",
    "# dfanno.dtypes =\n",
    "#   image_id     int64\n",
    "#   id           int64\n",
    "#   caption     object\n",
    "#   dtype: object\n",
    "\n",
    "dfimages = pd.DataFrame(data=data['images'])\n",
    "# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n",
    "# dfimages.dtypes =\n",
    "#   license           int64\n",
    "#   file_name        object\n",
    "#   coco_url         object\n",
    "#   height            int64\n",
    "#   width             int64\n",
    "#   date_captured    object\n",
    "#   flickr_url       object\n",
    "#   id                int64\n",
    "#   dtype: object\n",
    "## of above, am dropping useless columns\n",
    "dfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n",
    "\n",
    "## columns remaining in the dfs are:\n",
    "# dfanno columns are      image_id , id , caption\n",
    "#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n",
    "# dfimages columns are    file_name        , height ,  width , id\n",
    "#                         000000397133.jpg , 427    ,  640   , 397133\n",
    "\n",
    "## the captions are not ordered for each image and seem to be randomly placed\n",
    "\n",
    "dfanno.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179765</td>\n",
       "      <td>38</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179765</td>\n",
       "      <td>182</td>\n",
       "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179765</td>\n",
       "      <td>479</td>\n",
       "      <td>A black Honda motorcycle with a dark burgundy ...</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id   id                                            caption  \\\n",
       "0    179765   38  A black Honda motorcycle parked in front of a ...   \n",
       "1    179765  182      A Honda motorcycle parked in a grass driveway   \n",
       "2    179765  479  A black Honda motorcycle with a dark burgundy ...   \n",
       "\n",
       "          file_name  \n",
       "0  000000179765.jpg  \n",
       "1  000000179765.jpg  \n",
       "2  000000179765.jpg  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## marrying the \"annotations\" and the \"images\" keys info together into dfanno\n",
    "## the file_name column has the actual image name from the \"image\" key section\n",
    "dfanno = dfanno.merge(dfimages, how=\"inner\", left_on='image_id', right_on='id')\n",
    "dfanno.drop(['height', 'width', 'id_y'], axis = 1, inplace=True)\n",
    "dfanno.rename(columns={'id_x':'id'}, inplace=True)\n",
    "dfanno.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(descriptions_test) = 5000\n"
     ]
    }
   ],
   "source": [
    "## Mapping image with captions using dictionary\n",
    "\n",
    "def create_descriptions_dictionary(_dfin):\n",
    "    descriptions = {}\n",
    "    for row in _dfin.itertuples():\n",
    "      rowdict = row._asdict()\n",
    "      img_filename = rowdict['file_name'].split('.')[0] # drop the .jpg part\n",
    "      img_caption = rowdict['caption']\n",
    "      if(img_filename not in descriptions):\n",
    "        descriptions[img_filename] = [img_caption]\n",
    "      else:\n",
    "        descriptions[img_filename].append(img_caption)\n",
    "    return descriptions\n",
    "\n",
    "descriptions_test = create_descriptions_dictionary(dfanno)\n",
    "print(f\"len(descriptions_test) = {len(descriptions_test)}\")\n",
    "\n",
    "del dfanno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the description for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000179765': ['A black Honda motorcycle parked in front of a garage.',\n",
       "  'A Honda motorcycle parked in a grass driveway',\n",
       "  'A black Honda motorcycle with a dark burgundy seat.',\n",
       "  'Ma motorcycle parked on the gravel in front of a garage',\n",
       "  'A motorcycle with its brake extended standing outside'],\n",
       " '000000190236': ['An office cubicle with four different types of computers.',\n",
       "  'The home office space seems to be very cluttered.',\n",
       "  'an office with desk computer and chair and laptop.',\n",
       "  'Office setting with a lot of computer screens.',\n",
       "  'A desk and chair in an office cubicle.']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(descriptions_test.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A dog sitting between its masters feet on a footstool watching tv\\n',\n",
       " 'A dog between the feet of a person looking at a TV.',\n",
       " 'A dog and a person are watching television together.',\n",
       " 'A person is sitting with their dog watching tv.',\n",
       " 'A man relaxing at home, watching television with his dog.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of caption with accidental newline \\n in the caption\n",
    "descriptions_test['000000482917']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare translation table for removing punctuation\n",
    "## string.punctuation  gives   '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' and will take care of all these characters being made into a space\n",
    "tran_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "for key, desc_list in descriptions_test.items():\n",
    "    for idx in range(len(desc_list)):\n",
    "        desc = desc_list[idx]\n",
    "        # replace all punctuation with space in description before tokenizing\n",
    "        desc = desc.translate(tran_table)\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove any non-alphabetic tokens\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # overwrite with cleaned description\n",
    "        desc_list[idx] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog sitting between its masters feet on footstool watching tv',\n",
       " 'dog between the feet of person looking at tv',\n",
       " 'dog and person are watching television together',\n",
       " 'person is sitting with their dog watching tv',\n",
       " 'man relaxing at home watching television with his dog']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of caption with accidental newline \\n in the caption  -- POST CLEANUP\n",
    "descriptions_test['000000482917']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now the cleaned descriptions are ready in \"descriptions_test\" file.\n",
    "## Note: No need to add the startseq and endseq tokens as I will remove that from the inference before comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get an idea of the length of GT captions in cleaned descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black honda motorcycle parked in front of garage',\n",
       " 'honda motorcycle parked in grass driveway',\n",
       " 'black honda motorcycle with dark burgundy seat',\n",
       " 'ma motorcycle parked on the gravel in front of garage',\n",
       " 'motorcycle with its brake extended standing outside',\n",
       " 'an office cubicle with four different types of computers',\n",
       " 'the home office space seems to be very cluttered',\n",
       " 'an office with desk computer and chair and laptop']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gt_cap for desc in descriptions_test.values() for gt_cap in desc][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>caplen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>black honda motorcycle parked in front of garage</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>honda motorcycle parked in grass driveway</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>black honda motorcycle with dark burgundy seat</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ma motorcycle parked on the gravel in front of...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>motorcycle with its brake extended standing ou...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an office cubicle with four different types of...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the home office space seems to be very cluttered</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>an office with desk computer and chair and laptop</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  caplen\n",
       "0   black honda motorcycle parked in front of garage       8\n",
       "1          honda motorcycle parked in grass driveway       6\n",
       "2     black honda motorcycle with dark burgundy seat       7\n",
       "3  ma motorcycle parked on the gravel in front of...      10\n",
       "4  motorcycle with its brake extended standing ou...       7\n",
       "5  an office cubicle with four different types of...       9\n",
       "6   the home office space seems to be very cluttered       9\n",
       "7  an office with desk computer and chair and laptop       9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=[gt_cap for desc in descriptions_test.values() for gt_cap in desc], columns=['sent'])\n",
    "df['caplen'] = df['sent'].str.split().apply(len)\n",
    "#df['sent'][df['caplen'] == MAX_LENGTH_CAPTION]\n",
    "df[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45     1\n",
       "42     1\n",
       "35     1\n",
       "34     1\n",
       "33     1\n",
       "32     2\n",
       "30     1\n",
       "29     1\n",
       "28     2\n",
       "27     3\n",
       "26     1\n",
       "25     6\n",
       "24     6\n",
       "23     7\n",
       "22    11\n",
       "Name: caplen, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see counts of descriptions with the LONGEST lengths\n",
    "## we see only one caption has the maximum length\n",
    "## NOTE: no startseq and endseq tokens present\n",
    "countdf = df['caplen'].value_counts()\n",
    "countdf.sort_index(inplace=True, ascending=False)\n",
    "countdf.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5      160\n",
       "6     1971\n",
       "7     4873\n",
       "8     6005\n",
       "9     4864\n",
       "10    3118\n",
       "11    1764\n",
       "12     958\n",
       "13     512\n",
       "14     306\n",
       "15     151\n",
       "16     117\n",
       "17      61\n",
       "18      44\n",
       "19      26\n",
       "Name: caplen, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see counts of descriptions with the SHORTEST lengths\n",
    "## NOTE: no startseq and endseq tokens present\n",
    "countdf = df['caplen'].value_counts()\n",
    "countdf.sort_index(inplace=True, ascending=True)\n",
    "countdf.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## maximum lenght of description in Test data is 45 without extra tokens, else 47 with startseq and endseq inserted\n",
    "## But my trained model has max caption length of 49 including the extra tokens for startseq and endseq\n",
    "df['caplen'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>caplen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>large square concrete wall which shows people ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    sent  caplen\n",
       "15996  large square concrete wall which shows people ...      45"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see the description with longest length = 45\n",
    "df.loc[df['caplen'] == df['caplen'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, countdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGzQr4tiiWDc"
   },
   "source": [
    "## ALWAYS - Define the RNN Decoder model\n",
    "## Reload ALL weights from previous training point. Then freeze ONLY the embedding layer as before. Thus, the embedding layer will continue to use the GloVe-200 embeddings matrix weights that were first set during Training Phase 1 setup.\n",
    "\n",
    "\n",
    "### About Keras Emebedding layer\n",
    "### \n",
    "#### Default initializing of this layer: https://keras.io/api/layers/core_layers/embedding/\n",
    "######tf.keras.layers.Embedding(\n",
    "######    input_dim, output_dim,\n",
    "######    embeddings_initializer=\"uniform\",\n",
    "######    embeddings_regularizer=None,\n",
    "######    activity_regularizer=None,\n",
    "######    embeddings_constraint=None,\n",
    "######    mask_zero=False,\n",
    "######    input_length=None,\n",
    "######    **kwargs\n",
    "######)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run3/'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_WEIGHTS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Decoder_Run_3_Wt_ep_1.h5',\n",
       " 'Decoder_Run_3_Wt_ep_10.h5',\n",
       " 'Decoder_Run_3_Wt_ep_11.h5',\n",
       " 'Decoder_Run_3_Wt_ep_12.h5',\n",
       " 'Decoder_Run_3_Wt_ep_13.h5',\n",
       " 'Decoder_Run_3_Wt_ep_14.h5',\n",
       " 'Decoder_Run_3_Wt_ep_15.h5',\n",
       " 'Decoder_Run_3_Wt_ep_16.h5',\n",
       " 'Decoder_Run_3_Wt_ep_17.h5',\n",
       " 'Decoder_Run_3_Wt_ep_18.h5',\n",
       " 'Decoder_Run_3_Wt_ep_19.h5',\n",
       " 'Decoder_Run_3_Wt_ep_2.h5',\n",
       " 'Decoder_Run_3_Wt_ep_3.h5',\n",
       " 'Decoder_Run_3_Wt_ep_4.h5',\n",
       " 'Decoder_Run_3_Wt_ep_5.h5',\n",
       " 'Decoder_Run_3_Wt_ep_6.h5',\n",
       " 'Decoder_Run_3_Wt_ep_7.h5',\n",
       " 'Decoder_Run_3_Wt_ep_8.h5',\n",
       " 'Decoder_Run_3_Wt_ep_9.h5']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_WEIGHTS_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT - SPECIFY CORRECT WEIGHTS FILE TO LOAD FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD_WEIGHTS_FILE_NAME = r'Decoder_Run_3_Wt_ep_18.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will reload this file = /media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run3/Decoder_Run_3_Wt_ep_18.h5\n",
      "Verfiy the weights in file exists : Check value is True = True\n"
     ]
    }
   ],
   "source": [
    "RELOAD_WEIGHTS_FILE_PATH = IPDIR_WEIGHTS_IN + RELOAD_WEIGHTS_FILE_NAME\n",
    "print(f\"Will reload this file = {RELOAD_WEIGHTS_FILE_PATH}\")\n",
    "print(f\"Verfiy the weights in file exists : Check value is True = {os.path.exists(RELOAD_WEIGHTS_FILE_PATH) and os.path.isfile(RELOAD_WEIGHTS_FILE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameter_counts(_model):\n",
    "    total_params = _model.count_params()\n",
    "    total_trainable_params = int(np.sum([np.prod(v.get_shape().as_list()) for v in _model.trainable_variables]))\n",
    "    total_non_trainable_params = int(np.sum([np.prod(v.get_shape().as_list()) for v in _model.non_trainable_variables]))\n",
    "    #print(f\"Total params = {total_decoder_params}\\t Trainable total = {total_decoder_trainable_params}\\t Non-trainable total = {total_decoder_non_trainable_params}\")\n",
    "    return total_params, total_trainable_params, total_non_trainable_params\n",
    "\n",
    "def reload_rnn_decoder_for_inference(_saved_weights_file, _EMBEDDING_DIMS, _VOCAB_SIZE, _MAX_LENGTH_CAPTION):\n",
    "    if os.path.exists(_saved_weights_file) and os.path.isfile(_saved_weights_file):\n",
    "        ## Decoder Model defining\n",
    "        \n",
    "        ## parameters to define model - sent during function call\n",
    "        #EMBEDDING_DIMS is initialised earlier while creating embedding matrix\n",
    "        #VOCAB_SIZE is initialised earlier\n",
    "        #MAX_LENGTH_CAPTION is initialised earlier\n",
    "        \n",
    "        inputs1 = keras.Input(shape=(2048,))\n",
    "        fe1 = keras.layers.Dropout(0.5)(inputs1)\n",
    "        fe2 = keras.layers.Dense(256, activation='relu')(fe1)\n",
    "        \n",
    "        # partial caption sequence model\n",
    "        inputs2 = keras.Input(shape=(_MAX_LENGTH_CAPTION,))\n",
    "        se1 = keras.layers.Embedding(_VOCAB_SIZE, _EMBEDDING_DIMS, mask_zero=True)(inputs2)\n",
    "        se2 = keras.layers.Dropout(0.5)(se1)\n",
    "        se3 = keras.layers.LSTM(256)(se2)\n",
    "        \n",
    "        # decoder (feed forward) model\n",
    "        decoder1 = keras.layers.add([fe2, se3])\n",
    "        decoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\n",
    "        outputs = keras.layers.Dense(_VOCAB_SIZE, activation='softmax')(decoder2)\n",
    "        \n",
    "        # merge the two input models\n",
    "        reloaded_rnn_decoder_model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        \n",
    "        total_p, trainable_p, non_trainable_p = get_model_parameter_counts(reloaded_rnn_decoder_model)\n",
    "        print(f\"\\nCreated RNN Decoder model defined with these paramenters:\\nEMBEDDING_DIMS = {_EMBEDDING_DIMS} , VOCAB_SIZE = {_VOCAB_SIZE} , MAX_LENGTH_CAPTION = {_MAX_LENGTH_CAPTION}\")\n",
    "        print(f\"\\nBEFORE LOADING WEIGHTS AND FREEZING, parameter counts:\\nTotal parameters = {total_p} , Trainable parameter = {trainable_p} , Non-trainable parameters = {non_trainable_p}\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\nAttempting to load weights...\\n\")\n",
    "        \n",
    "        ## load the weights\n",
    "        reloaded_rnn_decoder_model.load_weights(_saved_weights_file)\n",
    "        print(f\"SUCCESS - Reloaded weights from :: {_saved_weights_file}\")\n",
    "        \n",
    "        ## freeze the embeddings layer weights so they are non-trainable\n",
    "        reloaded_rnn_decoder_model.trainable = False\n",
    "        print(f\"\\nFrozen all layers as this is only for inference and not for training.\")\n",
    "        \n",
    "        total_p, trainable_p, non_trainable_p = get_model_parameter_counts(reloaded_rnn_decoder_model)\n",
    "        print(f\"\\nAFTER FREEZING, parameter counts:\\nTotal parameters = {total_p} , Trainable parameter = {trainable_p} , Non-trainable parameters = {non_trainable_p}\")\n",
    "        \n",
    "        return reloaded_rnn_decoder_model\n",
    "    else:\n",
    "        print(f\"\\nERROR reloading weights. Check weights file exists here = {_saved_weights_file} ;\\nOR model setup parameters incompatible with the saved weights file given.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values to be used in Decoder setup:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "Weights to be reloaded from here:\n",
      "/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run3/Decoder_Run_3_Wt_ep_18.h5\n"
     ]
    }
   ],
   "source": [
    "print(f\"The values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\n",
    "print(f\"Weights to be reloaded from here:\\n{RELOAD_WEIGHTS_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created RNN Decoder model defined with these paramenters:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "\n",
      "BEFORE LOADING WEIGHTS AND FREEZING, parameter counts:\n",
      "Total parameters = 4146710 , Trainable parameter = 4146710 , Non-trainable parameters = 0\n",
      "\n",
      "Attempting to load weights...\n",
      "\n",
      "SUCCESS - Reloaded weights from :: /media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run3/Decoder_Run_3_Wt_ep_18.h5\n",
      "\n",
      "Frozen all layers as this is only for inference and not for training.\n",
      "\n",
      "AFTER FREEZING, parameter counts:\n",
      "Total parameters = 4146710 , Trainable parameter = 0 , Non-trainable parameters = 4146710\n",
      "\n",
      "<class 'tensorflow.python.keras.engine.training.Model'>\n"
     ]
    }
   ],
   "source": [
    "reloaded_RNN_decoder = reload_rnn_decoder_for_inference(RELOAD_WEIGHTS_FILE_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\n",
    "if reloaded_RNN_decoder is None:\n",
    "    print(f\"FATAL ERROR setting up Decoder\")\n",
    "else:\n",
    "    print(f\"\\n{type(reloaded_RNN_decoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "K_mLYJjsRqMO",
    "outputId": "809ebcdd-f972-4f47-f60f-a207404054a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 49, 200)      1351600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 49, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6758)         1736806     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,146,710\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,146,710\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n",
    "## NOTE the count of Trainable params has REDUCED\n",
    "reloaded_RNN_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() AFTER FREEZING EVERYTHING\n",
    "## NOTE the count of Trainable params is ZERO now\n",
    "\n",
    "#Model: \"functional_5\"\n",
    "#__________________________________________________________________________________________________\n",
    "#Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "#==================================================================================================\n",
    "#input_6 (InputLayer)            [(None, 49)]         0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#input_5 (InputLayer)            [(None, 2048)]       0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#embedding_2 (Embedding)         (None, 49, 200)      1351600     input_6[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_4 (Dropout)             (None, 2048)         0           input_5[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_5 (Dropout)             (None, 49, 200)      0           embedding_2[0][0]                \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_6 (Dense)                 (None, 256)          524544      dropout_4[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#lstm_2 (LSTM)                   (None, 256)          467968      dropout_5[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#add_2 (Add)                     (None, 256)          0           dense_6[0][0]                    \n",
    "#                                                                 lstm_2[0][0]                     \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_7 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_8 (Dense)                 (None, 6758)         1736806     dense_7[0][0]                    \n",
    "#==================================================================================================\n",
    "#Total params: 4,146,710\n",
    "#Trainable params: 0\n",
    "#Non-trainable params: 4,146,710\n",
    "#__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNo79p0Az37z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform inference on all the 5k images of the TEST DATASET (coco_val2017 data)\n",
    "## NOTE: For this data, the image encodings are already available so no need to start with the jpg image i.e. the CNN Encoder is not required at all.\n",
    "\n",
    "### descriptions_test dict has array of cleaned descriptions with image (without jpg) as key\n",
    "### img_encodings_test has the image encodings availabe with image (without jpg) as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{type(descriptions_test)}\")\n",
    "print(f\"{type(img_encodings_test)}\")\n",
    "## verify the key match\n",
    "print(f\"{ descriptions_test.keys() == img_encodings_test.keys() }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_encodings_test['000000179765'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to actually do the inference\n",
    "\n",
    "def greedySearch(_decoder_model, _img_encoding, _max_length, _wordtoix = None, _ixtoword = None):\n",
    "    wordtoix = _wordtoix\n",
    "    ixtoword = _ixtoword\n",
    "    in_text = 'startseq'\n",
    "    for i in range(_max_length):\n",
    "        sequence = [ wordtoix[w] for w in in_text.split() if w in wordtoix ]\n",
    "        sequence = keras.preprocessing.sequence.pad_sequences([sequence], maxlen=_max_length)\n",
    "        yhat = _decoder_model.predict([_img_encoding,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    caption_out = in_text.split()\n",
    "    #caption_out = caption_out[1:-1]  ## drop the startseq and endseq words at either end\n",
    "    caption_out = ' '.join(caption_out)\n",
    "    return caption_out\n",
    "\n",
    "def do_inference_one_image_on_encoding(_img_encoding_4m_pkl, _MAX_LENGTH_CAPTION, _model_RNN_decoder, _wordtoix, _ixtoword):\n",
    "    ## convert the shape from (2048,) to (1,2048) that the Decoder model expects\n",
    "    img_encoding_for_inference = _img_encoding_4m_pkl.reshape((1,2048))\n",
    "    \n",
    "    ## get the prediction caption using greedy search\n",
    "    predicted_caption = greedySearch(_model_RNN_decoder, img_encoding_for_inference, _MAX_LENGTH_CAPTION, _wordtoix, _ixtoword)\n",
    "    return predicted_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH_CAPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5000\n",
      "1 cap =\n",
      "startseq motorcycle parked on the side of the road endseq\n",
      "Processing 51 of 5000\n",
      "Processing 101 of 5000\n",
      "101 cap =\n",
      "startseq large clock tower with clock on it endseq\n",
      "Processing 151 of 5000\n",
      "Processing 201 of 5000\n",
      "201 cap =\n",
      "startseq kitchen with wooden floors and wooden table endseq\n",
      "Processing 251 of 5000\n",
      "Processing 301 of 5000\n",
      "301 cap =\n",
      "startseq an old stove with an open door and the door open endseq\n",
      "Processing 351 of 5000\n",
      "Processing 401 of 5000\n",
      "401 cap =\n",
      "startseq plane flying through the sky with clouds in the background endseq\n",
      "Processing 451 of 5000\n",
      "Processing 501 of 5000\n",
      "501 cap =\n",
      "startseq man in kitchen with his hands up endseq\n",
      "Processing 551 of 5000\n",
      "Processing 601 of 5000\n",
      "601 cap =\n",
      "startseq dog is sitting on the floor with frisbee endseq\n",
      "Processing 651 of 5000\n",
      "Processing 701 of 5000\n",
      "701 cap =\n",
      "startseq bathroom sink with mirror and mirror endseq\n",
      "Processing 751 of 5000\n",
      "Processing 801 of 5000\n",
      "801 cap =\n",
      "startseq small airplane is parked on the runway endseq\n",
      "Processing 851 of 5000\n",
      "Processing 901 of 5000\n",
      "901 cap =\n",
      "startseq giraffe standing in field with trees in the background endseq\n",
      "Processing 951 of 5000\n",
      "Processing 1001 of 5000\n",
      "1001 cap =\n",
      "startseq fire hydrant is sitting on the side of the road endseq\n",
      "Processing 1051 of 5000\n",
      "Processing 1101 of 5000\n",
      "1101 cap =\n",
      "startseq red and white bus is parked in front of building endseq\n",
      "Processing 1151 of 5000\n",
      "Processing 1201 of 5000\n",
      "1201 cap =\n",
      "startseq bird is standing on the edge of the water endseq\n",
      "Processing 1251 of 5000\n",
      "Processing 1301 of 5000\n",
      "1301 cap =\n",
      "startseq group of people standing around with their skateboards endseq\n",
      "Processing 1351 of 5000\n",
      "Processing 1401 of 5000\n",
      "1401 cap =\n",
      "startseq bench sitting in front of tree endseq\n",
      "Processing 1451 of 5000\n",
      "Processing 1501 of 5000\n",
      "1501 cap =\n",
      "startseq man sitting on bench with dog endseq\n",
      "Processing 1551 of 5000\n",
      "Processing 1601 of 5000\n",
      "1601 cap =\n",
      "startseq two sheep are standing in the grass endseq\n",
      "Processing 1651 of 5000\n",
      "Processing 1701 of 5000\n",
      "1701 cap =\n",
      "startseq boat is sitting on the water near the water endseq\n",
      "Processing 1751 of 5000\n",
      "Processing 1801 of 5000\n",
      "1801 cap =\n",
      "startseq train is parked on the side of the road endseq\n",
      "Processing 1851 of 5000\n",
      "Processing 1901 of 5000\n",
      "1901 cap =\n",
      "startseq bear is standing in the middle of the road endseq\n",
      "Processing 1951 of 5000\n",
      "Processing 2001 of 5000\n",
      "2001 cap =\n",
      "startseq cat is sitting on the floor looking at the camera endseq\n",
      "Processing 2051 of 5000\n",
      "Processing 2101 of 5000\n",
      "2101 cap =\n",
      "startseq man standing on snow covered slope next to red truck endseq\n",
      "Processing 2151 of 5000\n",
      "Processing 2201 of 5000\n",
      "2201 cap =\n",
      "startseq two men playing frisbee in field endseq\n",
      "Processing 2251 of 5000\n",
      "Processing 2301 of 5000\n",
      "2301 cap =\n",
      "startseq man with hat and hat on endseq\n",
      "Processing 2351 of 5000\n",
      "Processing 2401 of 5000\n",
      "2401 cap =\n",
      "startseq truck with graffiti on it is parked in the grass endseq\n",
      "Processing 2451 of 5000\n",
      "Processing 2501 of 5000\n",
      "2501 cap =\n",
      "startseq bowl of food with broccoli and meat endseq\n",
      "Processing 2551 of 5000\n",
      "Processing 2601 of 5000\n",
      "2601 cap =\n",
      "startseq dog is sitting on bench in front of the building endseq\n",
      "Processing 2651 of 5000\n",
      "Processing 2701 of 5000\n",
      "2701 cap =\n",
      "startseq man riding skateboard down the side of ramp endseq\n",
      "Processing 2751 of 5000\n",
      "Processing 2801 of 5000\n",
      "2801 cap =\n",
      "startseq baseball player swinging bat at ball endseq\n",
      "Processing 2851 of 5000\n",
      "Processing 2901 of 5000\n",
      "2901 cap =\n",
      "startseq baseball player swinging bat on field endseq\n",
      "Processing 2951 of 5000\n",
      "Processing 3001 of 5000\n",
      "3001 cap =\n",
      "startseq fruit stand with lots of vegetables and vegetables endseq\n",
      "Processing 3051 of 5000\n",
      "Processing 3101 of 5000\n",
      "3101 cap =\n",
      "startseq woman walking down street with an umbrella endseq\n",
      "Processing 3151 of 5000\n",
      "Processing 3201 of 5000\n",
      "3201 cap =\n",
      "startseq dog is standing on the grass with frisbee endseq\n",
      "Processing 3251 of 5000\n",
      "Processing 3301 of 5000\n",
      "3301 cap =\n",
      "startseq living room with television and couch endseq\n",
      "Processing 3351 of 5000\n",
      "Processing 3401 of 5000\n",
      "3401 cap =\n",
      "startseq man is holding surfboard on the beach endseq\n",
      "Processing 3451 of 5000\n",
      "Processing 3501 of 5000\n",
      "3501 cap =\n",
      "startseq man riding skis down snow covered slope endseq\n",
      "Processing 3551 of 5000\n",
      "Processing 3601 of 5000\n",
      "3601 cap =\n",
      "startseq man with hat and hat holding an umbrella endseq\n",
      "Processing 3651 of 5000\n",
      "Processing 3701 of 5000\n",
      "3701 cap =\n",
      "startseq man and woman are standing in the grass endseq\n",
      "Processing 3751 of 5000\n",
      "Processing 3801 of 5000\n",
      "3801 cap =\n",
      "startseq man in suit is holding up doughnut endseq\n",
      "Processing 3851 of 5000\n",
      "Processing 3901 of 5000\n",
      "3901 cap =\n",
      "startseq cake with candles on top of it endseq\n",
      "Processing 3951 of 5000\n",
      "Processing 4001 of 5000\n",
      "4001 cap =\n",
      "startseq woman laying on bed with her arms crossed endseq\n",
      "Processing 4051 of 5000\n",
      "Processing 4101 of 5000\n",
      "4101 cap =\n",
      "startseq man is playing tennis on the court endseq\n",
      "Processing 4151 of 5000\n",
      "Processing 4201 of 5000\n",
      "4201 cap =\n",
      "startseq an old cell phone sitting on table endseq\n",
      "Processing 4251 of 5000\n",
      "Processing 4301 of 5000\n",
      "4301 cap =\n",
      "startseq man playing tennis on tennis court endseq\n",
      "Processing 4351 of 5000\n",
      "Processing 4401 of 5000\n",
      "4401 cap =\n",
      "startseq an image of an old fashioned cake with candles on it endseq\n",
      "Processing 4451 of 5000\n",
      "Processing 4501 of 5000\n",
      "4501 cap =\n",
      "startseq woman is standing on beach with kite endseq\n",
      "Processing 4551 of 5000\n",
      "Processing 4601 of 5000\n",
      "4601 cap =\n",
      "startseq kitchen with sink and refrigerator endseq\n",
      "Processing 4651 of 5000\n",
      "Processing 4701 of 5000\n",
      "4701 cap =\n",
      "startseq an old fashioned computer keyboard sitting on top of table endseq\n",
      "Processing 4751 of 5000\n",
      "Processing 4801 of 5000\n",
      "4801 cap =\n",
      "startseq man in suit and tie talking on cell phone endseq\n",
      "Processing 4851 of 5000\n",
      "Processing 4901 of 5000\n",
      "4901 cap =\n",
      "startseq vase with flowers in it sitting on table endseq\n",
      "Processing 4951 of 5000\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "## use the encodings and get inferences\n",
    "comparison_dict = {}  ## key will be the image name, values will be inference caption and the cleaned descriptions\n",
    "total_to_infer = len(img_encodings_test)\n",
    "i = 0\n",
    "for key_img, img_pre_encoding_from_pkl in img_encodings_test.items():\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processing {i+1} of {total_to_infer}\")\n",
    "    ## get the inference caption\n",
    "    predicted_caption = do_inference_one_image_on_encoding(img_pre_encoding_from_pkl, MAX_LENGTH_CAPTION, reloaded_RNN_decoder, wordtoix, ixtoword)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i+1} cap =\\n{predicted_caption}\")\n",
    "    ## strip off the startseq and endseq - note that startseq is ALWAYS present as the first word, but the endseq MAY NOT ALWAYS be at the end\n",
    "    predicted_caption = predicted_caption.split(' ')\n",
    "    if predicted_caption[-1] == 'endseq':\n",
    "        predicted_caption = ' '.join(predicted_caption[1:-1]) ## remove both ends\n",
    "    else:\n",
    "        predicted_caption = ' '.join(predicted_caption[1:])   ## remove only startseq\n",
    "    ## update the comparison dict\n",
    "    comparison_dict.update( { key_img : [predicted_caption , descriptions_test[key_img] ] } )\n",
    "    i += 1\n",
    "print(f\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000179765': ['motorcycle parked on the side of the road',\n",
       "  ['black honda motorcycle parked in front of garage',\n",
       "   'honda motorcycle parked in grass driveway',\n",
       "   'black honda motorcycle with dark burgundy seat',\n",
       "   'ma motorcycle parked on the gravel in front of garage',\n",
       "   'motorcycle with its brake extended standing outside']],\n",
       " '000000190236': ['desk with computer and computer on it',\n",
       "  ['an office cubicle with four different types of computers',\n",
       "   'the home office space seems to be very cluttered',\n",
       "   'an office with desk computer and chair and laptop',\n",
       "   'office setting with lot of computer screens',\n",
       "   'desk and chair in an office cubicle']],\n",
       " '000000331352': ['toilet with white lid and sink',\n",
       "  ['small closed toilet in cramped space',\n",
       "   'tan toilet and sink combination in small room',\n",
       "   'this is an advanced toilet with sink and control panel',\n",
       "   'close up picture of toilet with fountain',\n",
       "   'off white toilet with faucet and controls']],\n",
       " '000000517069': ['man is walking down the street with his skateboard',\n",
       "  ['two women waiting at bench next to street',\n",
       "   'woman sitting on bench and woman standing waiting for the bus',\n",
       "   'woman sitting on bench in the middle of the city',\n",
       "   'woman sitting on bench and woman standing behind the bench at bus stop',\n",
       "   'woman and another woman waiting at stop']],\n",
       " '000000182417': ['plate of food with fork and fork',\n",
       "  ['beautiful dessert waiting to be shared by two people',\n",
       "   'there is piece of cake on plate with decorations on it',\n",
       "   'creamy cheesecake dessert with whip cream and caramel',\n",
       "   'an extravagant dessert on plate overlooking the water',\n",
       "   'this is picture of an extremely fancy desert']],\n",
       " '000000046378': ['cat is standing on the ground with its head on the ground',\n",
       "  ['cat eating bird it has caught',\n",
       "   'white cat caught bird outside on patio',\n",
       "   'grey house cat devours song bird on door step',\n",
       "   'long haired cat eating dead bird',\n",
       "   'cat eating dead bird on the ground']],\n",
       " '000000093437': ['man in black shirt and tie holding up hotdog',\n",
       "  ['shot of an elderly man inside kitchen',\n",
       "   'an old man is wearing an odd hat',\n",
       "   'an older man is wearing funny hat in his dining room',\n",
       "   'man in jacket and hat looks at the camera',\n",
       "   'an old man standing in kitchen posing for picture']],\n",
       " '000000172330': ['dog is sitting on the hood of car',\n",
       "  ['cat in between two cars in parking lot',\n",
       "   'cat stands between two parked cars on grassy sidewalk',\n",
       "   'cat at attention between two parked cars',\n",
       "   'grey and white cat watches from between parked cars',\n",
       "   'grey and white cat standing in the grass in parking lot']],\n",
       " '000000472678': ['computer desk with computer monitor and keyboard',\n",
       "  ['an office cubicle with multiple computers in it',\n",
       "   'an office desk with two flat panel monitors',\n",
       "   'an office desk with two computer screens books diagrams and phone on it',\n",
       "   'two computer monitors are placed beside each other on desk',\n",
       "   'desk with two monitors depicting security cameras']],\n",
       " '000000314251': ['man riding bike down street',\n",
       "  ['parade of motorcycles is going through group of tall trees',\n",
       "   'group of motorcyclists drive down tree lined street',\n",
       "   'group of motorcycles down long street filled with trees on either side',\n",
       "   'group of people riding mopeds through park',\n",
       "   'group of scooters rides down street']],\n",
       " '000000223747': ['man laying on bed with his dog',\n",
       "  ['man sleeping with his cat next to him',\n",
       "   'young man and his cute cat enjoy nap together',\n",
       "   'man is sleeping with his head on pillow',\n",
       "   'man sleeping in his bedroom next to cat',\n",
       "   'man sleeping next to cat on bed']],\n",
       " '000000109976': ['kitchen with stove sink and refrigerator',\n",
       "  ['an all white kitchen with an electric stovetop',\n",
       "   'white stove sits between two small counter tops',\n",
       "   'close up of white kitchen setup with coffee maker on counter',\n",
       "   'white stove and cabinet inside kitchen',\n",
       "   'this is small kitchen with white cabinets and appliances']],\n",
       " '000000012667': ['woman is holding pair of scissors',\n",
       "  ['the telephone has banana where the receiver should be',\n",
       "   'banana replacing the phone on an answering machine',\n",
       "   'phone with banana where the receiver should be',\n",
       "   'telephone has it receiver replaced with banana',\n",
       "   'banana placed on phone on table']],\n",
       " '000000482917': ['dog laying on the floor next to television',\n",
       "  ['dog sitting between its masters feet on footstool watching tv',\n",
       "   'dog between the feet of person looking at tv',\n",
       "   'dog and person are watching television together',\n",
       "   'person is sitting with their dog watching tv',\n",
       "   'man relaxing at home watching television with his dog']],\n",
       " '000000534605': ['group of people riding motorcycles down street',\n",
       "  ['man in motorcycle leathers standing in front of group of bikes',\n",
       "   'bikers dressed in their gear standing near their motorcycles',\n",
       "   'group of men stand next to their bicycles',\n",
       "   'three men standing around their motorcycles in parking lot',\n",
       "   'group of three motorcyclists standing in front of their motorcycles']]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see a few random entries\n",
    "dict(list(comparison_dict.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VERIFY WHERE TO PICKLE THE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPDIR = r'/home/rohit/PyWDUbuntu/thesis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rohit/PyWDUbuntu/thesis/'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARISON_DICT_PICKLE_FILE = r'ImgCap_Run3_TestSet_DescCap_Comparison_WtEp18_1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picle destination=\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCap_Run3_TestSet_DescCap_Comparison_WtEp18_1.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Picle destination=\\n{OPDIR + COMPARISON_DICT_PICKLE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(OPDIR + COMPARISON_DICT_PICKLE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, pickled to:\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCap_Run3_TestSet_DescCap_Comparison_WtEp18_1.pkl\n"
     ]
    }
   ],
   "source": [
    "## pickle it\n",
    "if os.path.exists(OPDIR + COMPARISON_DICT_PICKLE_FILE):\n",
    "    print(f\"File already exists - DID NOT PICKLE\")\n",
    "else:\n",
    "    with open(OPDIR + COMPARISON_DICT_PICKLE_FILE, 'wb') as handle:\n",
    "      pickle.dump(comparison_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"Success, pickled to:\\n{OPDIR + COMPARISON_DICT_PICKLE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['motorcycle parked on the side of the road',\n",
       " ['black honda motorcycle parked in front of garage',\n",
       "  'honda motorcycle parked in grass driveway',\n",
       "  'black honda motorcycle with dark burgundy seat',\n",
       "  'ma motorcycle parked on the gravel in front of garage',\n",
       "  'motorcycle with its brake extended standing outside']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## from original data - without reloading from pickle - for reference when reloading later\n",
    "comparison_dict['000000179765']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATE BLEU SCORE USING NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER IMPORTANT - CHANGE CONDA ENV FIRST\n",
    "## DO NOT USE               ce7comb1\n",
    "## CHANGE TO                ce6idelements1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as nltk_bleu\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rohit/PyWDUbuntu/thesis/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR = r'/home/rohit/PyWDUbuntu/thesis/'\n",
    "OPDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARISON_DICT_PICKLE_FILE = r'ImgCap_Run3_TestSet_DescCap_Comparison_WtEp18_1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picle source=\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCap_Run3_TestSet_DescCap_Comparison_WtEp18_1.pkl\n",
      "\n",
      "Exists = True\t\tIs File = True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Picle source=\\n{OPDIR + COMPARISON_DICT_PICKLE_FILE}\")\n",
    "print(f\"\\nExists = {os.path.exists(OPDIR + COMPARISON_DICT_PICKLE_FILE)}\\t\\tIs File = {os.path.isfile(OPDIR + COMPARISON_DICT_PICKLE_FILE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload from pickled file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded from picle file:\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCap_Run3_TestSet_DescCap_Comparison_WtEp18_1.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(OPDIR + COMPARISON_DICT_PICKLE_FILE, 'rb') as handle:\n",
    "  comparison_dict_reload = pickle.load(handle)\n",
    "print(f\"Reloaded from picle file:\\n{OPDIR + COMPARISON_DICT_PICKLE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['motorcycle parked on the side of the road',\n",
       " ['black honda motorcycle parked in front of garage',\n",
       "  'honda motorcycle parked in grass driveway',\n",
       "  'black honda motorcycle with dark burgundy seat',\n",
       "  'ma motorcycle parked on the gravel in front of garage',\n",
       "  'motorcycle with its brake extended standing outside']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compare reloaded info with - for reference displayed from original data\n",
    "comparison_dict_reload['000000179765']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu scores with NLTK\n",
    "\n",
    "### Link: https://ariepratama.github.io/Introduction-to-BLEU-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000179765': ['motorcycle parked on the side of the road',\n",
       "  ['black honda motorcycle parked in front of garage',\n",
       "   'honda motorcycle parked in grass driveway',\n",
       "   'black honda motorcycle with dark burgundy seat',\n",
       "   'ma motorcycle parked on the gravel in front of garage',\n",
       "   'motorcycle with its brake extended standing outside']],\n",
       " '000000190236': ['desk with computer and computer on it',\n",
       "  ['an office cubicle with four different types of computers',\n",
       "   'the home office space seems to be very cluttered',\n",
       "   'an office with desk computer and chair and laptop',\n",
       "   'office setting with lot of computer screens',\n",
       "   'desk and chair in an office cubicle']],\n",
       " '000000331352': ['toilet with white lid and sink',\n",
       "  ['small closed toilet in cramped space',\n",
       "   'tan toilet and sink combination in small room',\n",
       "   'this is an advanced toilet with sink and control panel',\n",
       "   'close up picture of toilet with fountain',\n",
       "   'off white toilet with faucet and controls']],\n",
       " '000000517069': ['man is walking down the street with his skateboard',\n",
       "  ['two women waiting at bench next to street',\n",
       "   'woman sitting on bench and woman standing waiting for the bus',\n",
       "   'woman sitting on bench in the middle of the city',\n",
       "   'woman sitting on bench and woman standing behind the bench at bus stop',\n",
       "   'woman and another woman waiting at stop']],\n",
       " '000000182417': ['plate of food with fork and fork',\n",
       "  ['beautiful dessert waiting to be shared by two people',\n",
       "   'there is piece of cake on plate with decorations on it',\n",
       "   'creamy cheesecake dessert with whip cream and caramel',\n",
       "   'an extravagant dessert on plate overlooking the water',\n",
       "   'this is picture of an extremely fancy desert']]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see a few random entries\n",
    "dict(list(comparison_dict_reload.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption 1 :: 000000179765.jpg :: Bleu score = 0.7690493285272233\n",
      "Caption 2 :: 000000190236.jpg :: Bleu score = 0.6865890479690392\n",
      "Caption 3 :: 000000331352.jpg :: Bleu score = 0.686987069307971\n",
      "Caption 4 :: 000000517069.jpg :: Bleu score = 0.42715068894185004\n"
     ]
    }
   ],
   "source": [
    "## see a few random bleu scores\n",
    "i = 0\n",
    "for k, v in comparison_dict_reload.items():\n",
    "    i += 1\n",
    "    #print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))\n",
    "    print(f\"Caption {i} :: {k + '.jpg'} :: Bleu score = {nltk_bleu.sentence_bleu(v[1], v[0])}\")\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/ce6idelements1/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 inputs\n"
     ]
    }
   ],
   "source": [
    "## calculate bleu scores for all the 5k images\n",
    "bscores_all = list()\n",
    "i = 0\n",
    "for k, v in comparison_dict_reload.items():\n",
    "    bscore = nltk_bleu.sentence_bleu(v[1], v[0])\n",
    "    bscores_all.append([k, v[0], bscore])\n",
    "    i += 1\n",
    "print(f\"Processed {i} inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0D3b7izSIqUb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bscores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['000000179765',\n",
       "  'motorcycle parked on the side of the road',\n",
       "  0.7690493285272233],\n",
       " ['000000190236', 'desk with computer and computer on it', 0.6865890479690392],\n",
       " ['000000331352', 'toilet with white lid and sink', 0.686987069307971],\n",
       " ['000000517069',\n",
       "  'man is walking down the street with his skateboard',\n",
       "  0.42715068894185004],\n",
       " ['000000182417', 'plate of food with fork and fork', 0.3898243978246645],\n",
       " ['000000046378',\n",
       "  'cat is standing on the ground with its head on the ground',\n",
       "  0.48564379670535185],\n",
       " ['000000093437',\n",
       "  'man in black shirt and tie holding up hotdog',\n",
       "  0.4698364460695633],\n",
       " ['000000172330', 'dog is sitting on the hood of car', 0.403069741728631],\n",
       " ['000000472678',\n",
       "  'computer desk with computer monitor and keyboard',\n",
       "  0.6924830567499759],\n",
       " ['000000314251', 'man riding bike down street', 0.5569727450591057]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see a few random entries\n",
    "bscores_all[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbs = pd.DataFrame(bscores_all, columns=['img', 'infcap', 'bsnltk'])\n",
    "dfbs.sort_values(by=['bsnltk'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>infcap</th>\n",
       "      <th>bsnltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>000000014888</td>\n",
       "      <td>man with hat and hat on</td>\n",
       "      <td>3.328384e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>000000099428</td>\n",
       "      <td>an old style style style style style style sty...</td>\n",
       "      <td>1.864381e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>000000489305</td>\n",
       "      <td>man is cutting cake with knife</td>\n",
       "      <td>1.267190e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>000000512929</td>\n",
       "      <td>man is cutting cake with knife</td>\n",
       "      <td>1.406807e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>000000397681</td>\n",
       "      <td>woman is holding bunch of fruit</td>\n",
       "      <td>1.412158e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               img                                             infcap  \\\n",
       "2300  000000014888                            man with hat and hat on   \n",
       "3745  000000099428  an old style style style style style style sty...   \n",
       "609   000000489305                     man is cutting cake with knife   \n",
       "4919  000000512929                     man is cutting cake with knife   \n",
       "3057  000000397681                    woman is holding bunch of fruit   \n",
       "\n",
       "            bsnltk  \n",
       "2300  3.328384e-78  \n",
       "3745  1.864381e-02  \n",
       "609   1.267190e-01  \n",
       "4919  1.406807e-01  \n",
       "3057  1.412158e-01  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>infcap</th>\n",
       "      <th>bsnltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>000000205105</td>\n",
       "      <td>man holding tennis racquet on tennis court</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>000000320696</td>\n",
       "      <td>man riding wave on top of surfboard</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4465</th>\n",
       "      <td>000000063602</td>\n",
       "      <td>laptop computer sitting on top of desk</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>000000023272</td>\n",
       "      <td>cat is sitting on the hood of car</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4907</th>\n",
       "      <td>000000356424</td>\n",
       "      <td>man sitting at table with plate of food</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               img                                      infcap  bsnltk\n",
       "4431  000000205105  man holding tennis racquet on tennis court     1.0\n",
       "3510  000000320696         man riding wave on top of surfboard     1.0\n",
       "4465  000000063602      laptop computer sitting on top of desk     1.0\n",
       "151   000000023272           cat is sitting on the hood of car     1.0\n",
       "4907  000000356424     man sitting at table with plate of food     1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBlue Scores:\n",
      "\n",
      "Max = 1.0\n",
      "Median = 0.6495123282211221\n",
      "Min = 3.3283835819065115e-78\n",
      "Average = 0.6426372693221797\n",
      "Std Dev = 0.16287133675674234\n"
     ]
    }
   ],
   "source": [
    "bs_max = float(dfbs[['bsnltk']].max(axis=0))\n",
    "bs_med = float(dfbs[['bsnltk']].median(axis=0))\n",
    "bs_min = float(dfbs[['bsnltk']].min(axis=0))\n",
    "bs_avg = float(dfbs[['bsnltk']].mean(axis=0))\n",
    "bs_std = float(dfbs[['bsnltk']].std(axis=0))\n",
    "print(f\"\\t\\tBlue Scores:\")\n",
    "print(f\"\\nMax = {bs_max}\\nMedian = {bs_med}\\nMin = {bs_min}\\nAverage = {bs_avg}\\nStd Dev = {bs_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfbs[dfbs['bsnltk'] > 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 to 0.05 : cnt= 2      cntCum= 2      relFreq= 0.00   cumFreq= 0.00   \n",
      "0.05 to 0.10 : cnt= 0      cntCum= 2      relFreq= 0.00   cumFreq= 0.00   \n",
      "0.10 to 0.15 : cnt= 3      cntCum= 5      relFreq= 0.00   cumFreq= 0.00   \n",
      "0.15 to 0.20 : cnt= 9      cntCum= 14     relFreq= 0.00   cumFreq= 0.00   \n",
      "0.20 to 0.25 : cnt= 24     cntCum= 38     relFreq= 0.00   cumFreq= 0.01   \n",
      "0.25 to 0.30 : cnt= 57     cntCum= 95     relFreq= 0.01   cumFreq= 0.02   \n",
      "0.30 to 0.35 : cnt= 106    cntCum= 201    relFreq= 0.02   cumFreq= 0.04   \n",
      "0.35 to 0.40 : cnt= 163    cntCum= 364    relFreq= 0.03   cumFreq= 0.07   \n",
      "0.40 to 0.45 : cnt= 265    cntCum= 629    relFreq= 0.05   cumFreq= 0.13   \n",
      "0.45 to 0.50 : cnt= 344    cntCum= 973    relFreq= 0.07   cumFreq= 0.19   \n",
      "0.50 to 0.55 : cnt= 480    cntCum= 1453   relFreq= 0.10   cumFreq= 0.29   \n",
      "0.55 to 0.60 : cnt= 508    cntCum= 1961   relFreq= 0.10   cumFreq= 0.39   \n",
      "0.60 to 0.65 : cnt= 540    cntCum= 2501   relFreq= 0.11   cumFreq= 0.50   \n",
      "0.65 to 0.70 : cnt= 619    cntCum= 3120   relFreq= 0.12   cumFreq= 0.62   \n",
      "0.70 to 0.75 : cnt= 566    cntCum= 3686   relFreq= 0.11   cumFreq= 0.74   \n",
      "0.75 to 0.80 : cnt= 462    cntCum= 4148   relFreq= 0.09   cumFreq= 0.83   \n",
      "0.80 to 0.85 : cnt= 339    cntCum= 4487   relFreq= 0.07   cumFreq= 0.90   \n",
      "0.85 to 0.90 : cnt= 231    cntCum= 4718   relFreq= 0.05   cumFreq= 0.94   \n",
      "0.90 to 0.95 : cnt= 142    cntCum= 4860   relFreq= 0.03   cumFreq= 0.97   \n",
      "0.95 to 1.00 : cnt= 88     cntCum= 4948   relFreq= 0.02   cumFreq= 0.99   \n"
     ]
    }
   ],
   "source": [
    "## freq distribution\n",
    "total_data = len(dfbs)\n",
    "step = np.linspace(0,1,21)[1] - np.linspace(0,1,21)[0]\n",
    "prev_running = 0\n",
    "for val in np.linspace(0,1,21)[1:]:   ## do not want to start from value 0.0 but from 0.05 till 1.0\n",
    "    wintop = round(float(val), 2)\n",
    "    winbot = wintop-step\n",
    "    print(f\"{winbot:.2f} to {wintop:.2f} : \", end='')\n",
    "    cnt_running = len(dfbs[ dfbs['bsnltk'] < wintop ] )\n",
    "    cnt_this_window = cnt_running - prev_running\n",
    "    print(f\"cnt= {cnt_this_window: <6} \", end='')\n",
    "    print(f\"cntCum= {cnt_running: <7}\", end='')\n",
    "    print(f\"relFreq= {cnt_this_window/total_data: <7.2f}\", end='')\n",
    "    print(f\"cumFreq= {cnt_running/total_data: <7.2f}\")\n",
    "    prev_running = cnt_running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = np.linspace(0,1,21)[1] - np.linspace(0,1,21)[0]\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
       "       0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0,1,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 ,\n",
       "       0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0,1,21)[4:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
